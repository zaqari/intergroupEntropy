{
 "cells": [
  {
   "cell_type": "markdown",
   "source": [
    "# Automating Entropy Calculations Across Examples"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "sys.path.append('..')\n",
    "\n",
    "import torch\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from datetime import datetime as dt\n",
    "from mod.entropy import entropy"
   ]
  },
  {
   "cell_type": "markdown",
   "source": [
    "### 1. Importing Data"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "location_path = \"data/\"\n",
    "vec_path = \"vecs/reddit-vecs.tsv\"\n",
    "\n",
    "df = pd.read_table(location_path + vec_path, lineterminator='\\n')\n",
    "\n",
    "g_col = 'subreddit'\n",
    "\n",
    "groups = df[g_col].unique()\n",
    "\n",
    "# Quick corpus details\n",
    "print(groups)\n",
    "print(list(df))\n",
    "for subreddit in df[g_col].unique():\n",
    "    print('{} \\t {} {}'.format(subreddit, len(df['_id'].loc[df[g_col].isin([subreddit])].unique()), df[g_col].isin([subreddit]).sum()))\n",
    "\n",
    "# select down to only the groups we care about, and sort by date to unscramble cross-post differences.\n",
    "df = df.sort_values(by=[g_col]+['comment_created_at', '_id'])\n",
    "df.index=range(len(df))"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "And we'll now reformat the vectors to be the correct format--i.e. torch vectors."
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "def revectorize(x):\n",
    "    x_ = str(x).replace('[', '').replace(']', '')\n",
    "    return np.array(x_.split(', ')).astype(float)\n",
    "\n",
    "begin = dt.now()\n",
    "df['vec'] = df['vec'].apply(lambda x: revectorize(x))\n",
    "Eu = df['vec'].values\n",
    "del df['vec']\n",
    "\n",
    "Eu = torch.FloatTensor(np.concatenate(Eu, axis=0))\n",
    "print('{} vecs made in {}\\n'.format(Eu.shape[0], dt.now()-begin))"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "And for accounting, we add a column 'n' to track the number of tokens in a sentence (per Torch)"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "_idn = df['_id'].value_counts()\n",
    "df['n'] = df['_id'].apply(lambda x: _idn[x])"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "### 2. Analyzing the data"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "First, let's set up a model class."
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "H = entropy().cuda()"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "And as with our word vectors, we set up a new document to stream our outputs to."
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "import os\n",
    "\n",
    "output_path = \"summaries/H-posteriors.csv\"\n",
    "\n",
    "meta_data_cols = [\n",
    "    '_id',\n",
    "    'subreddit',\n",
    "    'comment_ups',\n",
    "    'user',\n",
    "    'comment_created_at',\n",
    "    'sub_id'\n",
    "]\n",
    "\n",
    "dfposteriors = pd.DataFrame(columns=['x', 'y', 'xtime', 'ytime', 'n']+['x_'+col for col in meta_data_cols]+['y_'+col for col in meta_data_cols]+['H'])\n",
    "\n",
    "if not bool(os.listdir('summaries')):\n",
    "    dfposteriors.to_csv(location_path + output_path, index=False, encoding='utf-8')"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "And now we can run our model over the data and save the outputs.\n",
    "\n",
    "We'll start by creating a list of all permissable combinations."
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "from itertools import combinations\n",
    "total_combinations = combinations(df['_id'].unique(), 2)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "And then run our calculations."
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "begin = dt.now()\n",
    "with torch.no_grad():\n",
    "    for k, (i,j) in enumerate(total_combinations):\n",
    "        try:\n",
    "            xsel = df['_id'].isin([i]).values\n",
    "            ex = Eu[xsel]\n",
    "            xt = df['comment_created_at'].loc[xsel].values[0]\n",
    "\n",
    "            ysel = df['_id'].isin([j]).values\n",
    "            ey = Eu[ysel]\n",
    "            yt = df['comment_created_at'].loc[ysel].values[0]\n",
    "\n",
    "            Hij, Hji = H(ex.cuda(), ey.cuda())\n",
    "            Hij, Hji = Hij.detach().cpu().item(), Hji.detach().cpu().item()\n",
    "\n",
    "            df_ij = [\n",
    "                [i, j, xt, yt, ex.shape[0]]+df[meta_data_cols].loc[xsel].tolist()[0]+df[meta_data_cols].loc[ysel].tolist()[0]+[Hij],\n",
    "                [j, i, yt, xt, ey.shape[0]]+df[meta_data_cols].loc[ysel].tolist()[0]+df[meta_data_cols].loc[xsel].tolist()[0]+[Hji]\n",
    "            ]\n",
    "\n",
    "            df_ij = np.array(df_ij)\n",
    "            df_ij = pd.DataFrame(df_ij, columns=list(dfposteriors))\n",
    "            df_ij.to_csv(location_path+output_path, index=False, header=False, mode='a', encoding='utf-8')\n",
    "\n",
    "            if ((k+1) % int(len(total_combinations)/10)) == 0:\n",
    "                print('combo {}/{}, {}'.format(k+1, len(total_combinations), dt.now()-begin))\n",
    "\n",
    "        except Exception as ERR:\n",
    "            print(ERR)\n",
    "            print(i,j, '\\n')\n",
    "\n",
    "print('-------+++-------')"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}