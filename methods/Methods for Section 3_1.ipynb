{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true,
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "# Convergence as Entropy Minimization Across Lexico-Semantic Choice"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1 A Theory of Diachronic Language Shift within Groups\n",
    "\n",
    "Linguistic alignment/Convergence both describe the tendency for groups to converge on similar means of discussing a topic.\n",
    "\n",
    "Similar means can be expressed as a minimization in the entropy between utterances made by group members. As group members A and B sound more similar to one another, you can recover more of a group member A's semantic content from the lexical items in member B's message/utterance/sentence, because you can better predict member A's message just by listening to member B. In other words because they're using similar language to say the same thing the predictability of one utterance when presented with another utterance increases and thus entropy (i.e. how unpredictable two things are based on obersrvations of one or the other) decreases.\n",
    "\n",
    "Similarity in the precise lexico-semantic meaning of two words can be measured using contextual word embeddings. Models like BERT provide contextually informed word embeddings.\n",
    "\n",
    "The following is validation on toy data that, yes, there is indeed a difference in the entropy between messages written by members of the same group when compared to members of a different group. We validate this on a subset of 40 post titles discussing COVID-19, evenly sampled from subreddits \"r/covidiots\" and \"r/conspiracy\". No pre-processing was performed on the data."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 1.1 Generating intial data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "from data.corpus import *"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 1.2 Word vector representations\n",
    "\n",
    "$$ E_{xi} = wv(i \\in x) $$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "'python3 ./data/extremismVecsv5.py'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 1.3 Probability based on word vectors\n",
    "\n",
    "Imagine that an interlocutor is playing a kind of language reconstruction game. The interlocutor is given a single utterance from an individual, broken up into tokens. The interlocutor is then given a set of utterances also broken up into tokens from several utterances all taken from a number of members of some group. The interlocutor is then asked to take the groups' tokens and reconstruct an utterance that means something similar to the sentence they observed from the individual. This process can be repeated for the same original utterance using tokens from several different groups. In this scenario, reconstructed utterances that are more similar in meaning to the original utterance will have lower entropy. Reconstructed utterances that are either less similar or less intelligible will have higher entropy.\n",
    "\n",
    "We operationalize this language game by calculating entropy for utterances using BERT word vectors (Devlin et al. 2019)) to represent each token. This allows us to capture similarity between tokens that are semantically similar but are not a 1:1 mapping of the same word. Let $E_{xi}$ be the set of BERT word vectors for each token $w_i$ in a sentence $x$.\n",
    "\n",
    "$$E_{xi} = BERT(w_i \\in x)$$\n",
    "\n",
    "the probability that two words are semantically similar to one another based on their word vectors is a function of their location in vector space (Devlin et al., 2019; Mikolov et al., 2013; Pennington et al., 2014). If a word vector were a point in space, words that are more semantically related to one another will be closer to one another. We use cosine similarity (CoS) to calculate the proximity between word vectors. Now, the probability of two word vectors meaning the same thing can be thought of in the following way: if word vectors put words that are more semantically similar to one another closer in space, the probability that a word/token $i$ from a sentence $x$ is semantically similar to a word/token $j$ from a sentence $y$ can be thought of colloquially as how likely you are to hit $xi$ if you were to throw a dart at $yj$. We quantify this intuition about probability and vector space in equation 1 using a Gaussian distribution with a location parameter $\\mu=1.$ such that as the CoS value for the comparison of two word vectors approaches 1 we have maximum confidence that the two words mean the same thing, and a scale parameter $\\sigma$.\n",
    "\n",
    "$$P(E_{xi} | E_{yj}) = P_{\\mathcal{N}}\\left( CoS(E_{xi},E_{yj}) \\bigg|  \\mu=1, \\sigma \\right)$$\n",
    "\n",
    "Think of $\\sigma$ like the accuracy of the dart thrower, where lower $\\sigma$ values equate to the dart thrower only hitting a word/token $xi$ if it is very close to $yj$ in word vector space.\n",
    "\n",
    "However, we almost never have a reason to compare any one vector from a sentence $xi$ to any single vector from another sentence/distribution, $yj$. Instead, it’s better to ask how likely is a vector $xi$ conditioned on what we know about the total distribution $y$, in which there are $j$ tokens ($j \\in y$). A priori, one way of posing this question is by asking “when we compare $xi$ to the entirety of the distribution $y$, which token $j \\in y$ returns the maximum likelihood for $xi$ and what is the probability of $xi$ conditioned on that token?” We thus rewrite equation 1 as follows:\n",
    "\n",
    "\n",
    "$$P(E_{xi} | E_{y}) = P_{\\mathcal{N}} \\left( \\max_{j} \\left(CoS(E_{xi},E_{y}) \\right) \\bigg|  \\mu=1, \\sigma \\right)$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['__id', 'created_at', 'url', 'replies', 'retweets', 'favorites', 'user', 'bioguide_id', 'party', 'state', 'tokens', 'vecs']\n",
      "D    12339\n",
      "R    11124\n",
      "Name: party, dtype: int64\n"
     ]
    },
    {
     "data": {
      "text/plain": "torch.Size([23463, 1536])"
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import torch\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "s_col = 'subreddit_name'\n",
    "\n",
    "def vec_from_string(dfi):\n",
    "    return torch.FloatTensor(\n",
    "        [\n",
    "            [\n",
    "                float(i) for i in vec.replace('[', '').replace(']','').split(', ')\n",
    "            ] for vec in dfi['vecs'].values\n",
    "        ]\n",
    "    )\n",
    "\n",
    "df = pd.read_table(\"/Volumes/ROY/comp_ling/datasci/intergroupEntropy/data/senators458/vecs_538senators-tweets.tsv\")\n",
    "df['party'] = df['party'].replace({'I': 'D'})\n",
    "print(list(df))\n",
    "print(df['party'].value_counts())\n",
    "\n",
    "Eu = vec_from_string(df)\n",
    "Eu.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Because of how many comparisons there are to make, I had to break the algorithm up. Below we calculate the Cosine Similarity (CoS) for each vector compared to every other vector in the entirety of the dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "from shared.mod.mutual_information.RQA import *\n",
    "\n",
    "H = hRQA()\n",
    "H.streamCOS(Eu,Eu)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "From there we save that to make later calculations faster."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "torch.save(\n",
    "    {\n",
    "        'M':H.M,\n",
    "        'df': df[[i for i in list(df) if i not in ['vecs','vec']]],\n",
    "    },\n",
    "    \"/Volumes/ROY/comp_ling/datasci/intergroupEntropy/data/senators458/gun-control-ckpt.pt\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 1.4 Entropy across sentences using probability based on their component word vectors\n",
    "\n",
    "Meanwhile, the probabililty that an individual's message $x$ exhibits convergence with a historical, known individual/memger of group $h \\in H$ based on their message history is based on (1) how much of $x$'s history, per token, is recoverable from $h$'s history, which is expressed by (2) the amount of information relayed by the closest related lexical item $hj$ to each token $xi$.\n",
    "\n",
    "$$H( x ; y ) = -\\sum_i P(E_{xi}|E_{y}) \\log P(E_{xi}|E_{y})$$\n",
    "\n",
    "In the diachronic case, a message $x$ can only be similar to messages in $h$ that were written prior to $x$. Thus, let $\\tau_{x}$ be the time of message $x$, and let $E_{hj}$ contain only tokens pulled from messages occurring prior to $\\tau_x$--i.e.\n",
    "\n",
    "$$E_{hj} = \\bigg\\lbrace wv(j \\in h)\\delta_{\\tau_j < \\tau_x}\\,\\ .\\ .\\ .\\ wv(n \\in h)\\delta_{\\tau_n < \\tau_x}  \\bigg\\rbrace$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "First, I loaded the data from the checkpoint described above."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "D    12339\n",
      "R    11124\n",
      "Name: party, dtype: int64 \n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "s_col = 'party'\n",
    "ckpt = torch.load(\"/Volumes/ROY/comp_ling/datasci/intergroupEntropy/data/senators458/gun-control-ckpt.pt\")\n",
    "\n",
    "M, df = ckpt['M'], ckpt['df']\n",
    "ids = df['__id'].values\n",
    "df['party'] = df['party'].replace({'I': 'D'})\n",
    "print(df[s_col].value_counts(),'\\n\\n')\n",
    "del ckpt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['__id', 'created_at', 'url', 'replies', 'retweets', 'favorites', 'user', 'bioguide_id', 'party', 'state', 'tokens']\n",
      "D \t 334\n",
      "R \t 289\n"
     ]
    }
   ],
   "source": [
    "print(list(df))\n",
    "for subreddit in df[s_col].unique():\n",
    "    print('{} \\t {}'.format(subreddit, len(df['__id'].loc[df[s_col].isin([subreddit])].unique())))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "comm = np.array([df[s_col].loc[df['__id'].isin([idx])].unique()[0] for idx in np.unique(ids)])\n",
    "\n",
    "#Establish parameters for Gaussian\n",
    "d = torch.distributions.Normal(1,.3,validate_args=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 1.5 Assessing entropic differences between groups\n",
    "\n",
    "Monte Carlo sampling for two conditions: within grouping and outside of group.\n",
    "\n",
    "We break down our hypotheses as follows:\n",
    "\n",
    "- (H1) There is statistically significant, lower entropy between any sampled set of comments posted within a subreddit A comprised of like-minded individuals compared to any sampled set of comments posted to a subreddit B in opposition to A.\n",
    "- (H0) There is no statistically significant difference in entropy between two diametrically opposed subreddits.\n",
    "\n",
    "To test this, picture the following ``game''. Pretend that for each comment in oyur dataset you randomly sample $N$ comments at random from the same subreddit A, and $N$ comments from the opposing subreddit B. We then calculate the entropy between our comment and each comment in the random sample from A, as well as the entropy between our comment and each comment in the random sample from B. We then take the mean entropy between our original comment and the comments from A, and the comments from B separately for comparison. We repeat this process a number of times. Hypothesis H1 is validated if in greater than 5\\% ($\\alpha$) of cases there is lower mean entropy for our initial comment and sample A than there is for our initial comment and sample B. This test is significant if the distribution of means with A and the distribution for means with B is significantly different. Both of these conditions must be true in order to validate H1."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### A Permutation test from sampled message histories"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "permutation 100/1000\n",
      "permutation 200/1000\n",
      "permutation 300/1000\n",
      "permutation 400/1000\n",
      "permutation 500/1000\n",
      "permutation 600/1000\n",
      "permutation 700/1000\n",
      "permutation 800/1000\n",
      "permutation 900/1000\n",
      "permutation 1000/1000\n"
     ]
    }
   ],
   "source": [
    "# (0) negative surprisal\n",
    "# Hxy = d.log_prob(M)\n",
    "\n",
    "# (1) monte carlo procedure\n",
    "N_permutations, xsize, ysize = 1000, 20, 20\n",
    "\n",
    "sample_sets = np.array([[j for j in np.unique(ids) if j != i] for i in np.unique(ids)])\n",
    "comm_status = comm[sample_sets]\n",
    "uids = np.array([df['user'].loc[df['__id'].isin([idx])].unique()[0] for idx in np.unique(ids)])\n",
    "uidsset = uids[sample_sets]\n",
    "\n",
    "\n",
    "ML, MR = [],[]\n",
    "for permutation in range(N_permutations):\n",
    "\n",
    "    xml = np.random.choice(np.unique(ids)[comm=='D'], size=(xsize,), replace=False)\n",
    "    xmr = np.random.choice(np.unique(ids)[comm=='R'], size=(xsize,), replace=False)\n",
    "\n",
    "    idxml = df['__id'].isin(xml).values\n",
    "    idxmr = df['__id'].isin(xmr).values\n",
    "\n",
    "    mxml = torch.FloatTensor([df['__id'].values[idxml]==idx for idx in xml])\n",
    "    mxmr = torch.FloatTensor([df['__id'].values[idxmr]==idx for idx in xmr])\n",
    "\n",
    "    ysamples = {idx:\n",
    "        (\n",
    "        np.random.choice(sample_sets[idx][(comm_status[idx] == 'D') & (uidsset[idx] != uids[idx])], size=(ysize,), replace=False),\n",
    "        np.random.choice(sample_sets[idx][(comm_status[idx] == 'R') & (uidsset[idx] != uids[idx])], size=(ysize,), replace=False)\n",
    "        ) for idx in xml.tolist()+xmr.tolist()}\n",
    "\n",
    "    ysamples ={k: (df['__id'].isin(v[0]).values, df['__id'].isin(v[1]).values) for k,v in ysamples.items()}\n",
    "\n",
    "    ml, mr = [],[]\n",
    "    for x in xml:\n",
    "        xi = M[ids==x]\n",
    "        ml += [torch.cat([xi[:,ysamples[x][0]].max(dim=-1).values.unsqueeze(-1),\n",
    "                          xi[:,ysamples[x][1]].max(dim=-1).values.unsqueeze(-1)], dim=-1)]\n",
    "\n",
    "    for x in xmr:\n",
    "        xi = M[ids == x]\n",
    "        mr += [torch.cat([xi[:, ysamples[x][0]].max(dim=-1).values.unsqueeze(-1),\n",
    "                          xi[:, ysamples[x][1]].max(dim=-1).values.unsqueeze(-1)], dim=-1)]\n",
    "\n",
    "    ml, mr = torch.cat(ml, dim=0), torch.cat(mr, dim=0)\n",
    "    ml, mr = d.log_prob(ml), d.log_prob(mr)\n",
    "    ml, mr = -(torch.exp(ml) * ml), -(torch.exp(mr) * mr)\n",
    "    ml, mr = (mxml.unsqueeze(-1) * ml).sum(dim=1), (mxmr.unsqueeze(-1) * mr).sum(dim=1)\n",
    "\n",
    "    ML += [ml]\n",
    "    MR += [mr]\n",
    "\n",
    "    if ((permutation+1) % 100) == 0:\n",
    "        print('permutation {}/{}'.format(permutation+1, N_permutations))\n",
    "\n",
    "ML, MR = torch.cat(ML,dim=0), torch.cat(MR,dim=0)\n",
    "\n",
    "# Printing means and medians as test of normalcy\n",
    "# (ML[:,0].mean(), ML[:,0].median()), (ML[:,1].mean(), ML[:,1].median()), (MR[:,0].mean(), MR[:,0].median()), (MR[:,1].mean(), MR[:,1].median())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "outputs": [
    {
     "data": {
      "text/plain": "((tensor(3.9768), tensor(4.0181), tensor(4.0593)),\n (tensor(5.3929), tensor(5.4332), tensor(5.4735)),\n (tensor(5.9604), tensor(6.0090), tensor(6.0575)),\n (tensor(5.5860), tensor(5.6366), tensor(5.6872)))"
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def CI(x,z=1.96):\n",
    "    mu = x.mean()\n",
    "    side = z * (x.std()/np.sqrt(x.shape[0]))\n",
    "    return mu-side, mu, mu+side\n",
    "\n",
    "CI(ML[:,0]), CI(ML[:,1]), CI(MR[:,0]), CI(MR[:,1])"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "And testing the statistical significance of these distributions . . ."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": "(Ttest_indResult(statistic=-48.08948160400127, pvalue=0.0),\n Ttest_indResult(statistic=-10.406199276034744, pvalue=2.502825813965209e-25))"
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from scipy.stats import ttest_ind as ttest\n",
    "\n",
    "ttest(ML[:,0].numpy(), ML[:,1].numpy()),ttest(MR[:,1].numpy(), MR[:,0].numpy())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2 Conclusions\n",
    "\n",
    "What's provided here is a mock up example with real (albeit a fractional amount) data validating that within group communication exhibits lower entropy than intergroup communication. The methods proposed here can be used in a variety of applications, ranging from quantifying convergence within groups without prior definition of lexical dictionaries to, per person, measuring convergence with normative group communication styles over time (message-by-message)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "#### References\n",
    "\n",
    "Adams, A., Miles, J., Dunbar, N. E., & Giles, H. (2018). Communication accommodation in text messages: Exploring liking, power, and sex as predictors of textisms. The Journal of Social Psychology, 158(4), 474–490. https://doi.org/10.1080/00224545.2017.1421895\n",
    "\n",
    "Dale, R., Duran, N. D., & Coco, M. (2018). Dynamic Natural Language Processing with Recurrence Quantification Analysis. ArXiv:1803.07136 [Cs]. http://arxiv.org/abs/1803.07136\n",
    "\n",
    "de Vries, W., van Cranenburgh, A., & Nissim, M. (2020). What’s so special about BERT’s layers? A closer look at the NLP pipeline in monolingual and multilingual models. Findings of the Association for Computational Linguistics: EMNLP 2020, 4339–4350\n",
    "\n",
    "Palomares, N., Giles, H., Soliz, J., & Gallois, C. (2016). Intergroup Accommodation, Social Categories, and Identities. In H. Giles (Ed.), Communication Accomodation Theory (p. 232).\n",
    "\n",
    "Rosen, Z. (2022). A BERT’s eye view: A “big-data” framework for assessing language convergence and accommodation in large, many-to-many settings. Journal of Language and Social Psychology, 0261927X2210811."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}