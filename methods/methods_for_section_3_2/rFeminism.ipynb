{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true,
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "# Convergence as Entropy Minimization Across Lexico-Semantic Choice"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1 Communication Accommodation Theory\n",
    "\n",
    "Lexical alignment/Convergence both describe the tendency for members of a group to converge on similar means of discussing a topic.\n",
    "\n",
    "Similar means can be expressed as a minimization in the entropy between utterances made by group members. As group members A and B sound more similar to one another, you can recover more of a group member A's semantic content from the lexical items in member B's message/utterance/sentence, because you can better predict member A's message just by listening to member B. In other words because they're using similar language to say the same thing the predictability of one utterance when presented with another utterance increases and thus entropy (i.e. how unpredictable two things are based on obersrvations of one or the other) decreases.\n",
    "\n",
    "Similarity in the precise lexico-semantic meaning of two words can be measured using contextual word embeddings. Models like BERT (and it's twin RoBERTa) provide contextually informed word embeddings.\n",
    "\n",
    "The following is a quick analysis attempting to recover Social Identity attributes for speakers based on imputed convergence with another group.\n",
    "\n",
    "Specifically: recovering political party affiliation from tweets discussing immigrants."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 1.1 Generating intial data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "from intergroupEntropy.data.redditany.redo_collection_corpus import *"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 1.2 Word vector representations\n",
    "\n",
    "$$ E_{xi} = wv(i \\in x) $$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "'ssh -'\n",
    "'tmux attach-session -s BERT'\n",
    "'python3 ./reddit_vecs.py'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 1.3 Probability based on word vectors\n",
    "\n",
    "Based on our understanding of how to test conceptual similarities between individuals' utterances and groups', and to get a grip on our BERT-based method, let's imagine that an interlocutor is playing a kind of language reconstruction game that is directly related to the question posed in our description of the Bernoulli process. The interlocutor is given a single utterance from an individual $x$, broken up into tokens $xi$. The interlocutor is then given a set of tokens $j$ from several utterances all taken from a number of members of some group--$y j$. The interlocutor is then asked to take the groups' tokens $yj$ and reconstruct an utterance that means something that is as similar as possible to the sentence $x$. If they can reconstruct a sentence from the groups' tokens $yj$ that means something similar to the utterance $x$, this would effectively answer the question of whether or not\n",
    "\"for each token ($xi$) in the sentence $x$, tell me if someone in the sample $y$ used the same word, or a synonym for it, in the same way that it was used in $x$.\"\n",
    "Furthermore, in this scenario, reconstructed utterances that are more similar in meaning to the original utterance will have lower entropy. Reconstructed utterances that are either less similar or less intelligible will have higher entropy.\n",
    "\n",
    "We start our language game by, first, converting all of the tokens in both $x$ and $y$ to BERT word vectors (Devlin et al. 2019. This will allow us to capture similarity between tokens that are semantically similar but are not a 1:1 mapping of the same word. Let $E_{xi}$ be the set of BERT word vectors for each token $i$ in a sentence $x$ and $E_{yj}$ be the set of BERT word vectors for each token $j$ in a sample $y$ of utterances from a group. The equation \\ref{eq:bert} below shows the process of converting tokens $i \\in x$ to word vectors. Tokens $j \\in y$ are converted to word vectors via the same process.\n",
    "\n",
    "$$E_{xi} = BERT(i \\in x)$$\n",
    "\n",
    "The utility of word vector models is that they represent the meaning of words spatially and, surprisingly, accurately. Even in early, contextually uninformed models, words that are semantically similar to one another based on their word vectors cluster closer together in word vector space (GLoVe: Pennington et al. 2014; Word2Vec: Mikolov et al. 2013; BERT: Devlin et al. 2019). In contextually aware models like BERT and all subsequent transformer models words that have similar word senses cluster separately from other word senses. This allows us to make fine grain distinctions between the different meanings of polysemous words like the many meanings of \"bank, but it also allows us to capture subtle community/group-specific differences in word usage like the differences in the use of the word \"slay\" in example \\ref{} (Devlin et al. 2019). In layman's terms, if a word vector represents the meaning of a word as a point in space, words that are more semantically related to one another will be closer to one another. And if those word vectors are generated by a contextually aware model the closer \\textit{the word senses} of two words are to one another the closer two word vectors will be to one another in vector space. A popular way to measure the proximity of two word vectors to one another is to use Cosine Error (CoE), where a CoE value of 0 indicates that the word vectors for two words in high dimensional space are in a superposition of one another, and 2 means that they are maximally divergent.\n",
    "\n",
    "Think of finding similar words in word vector space like a game of darts, where CoE values that are closer to 0 when comparing a word vector $E_{xi}$ to another word vector $E_{yj}$ indicate that if you threw a dart at $E_{xi}$ you are more likely to accidentally hit the word vector $E_{yj}$ if you miss.\n",
    "\n",
    "Please note however that \\textit{proximity} in vector space is different from a probability, and CoE values are just a scaled measurement of proximity, not the probability that two vectors are the same or similar. An additional step is needed to render CoE values as probabilities that can be used as part of a statistical framework. To convert CoE to a probability, we leverage a half-Gaussian distribution, continuous on an interval of 0 to infinity, with two parameters: (1) a location parameter $\\mu=0.0$ such that as the CoE value for the comparison of two word vectors approaches 0 we have maximum confidence that the two words mean the same thing, and (2) a scale parameter $\\sigma$ that sets a penalty weight for CoE values farther away from 0.\n",
    "$$P(E_{xi} | E_{yj}) = P_{\\mathcal{N}_{[0,\\infty]}}\\left( CoE(E_{xi},E_{yj}) \\bigg|  \\mu=0., \\sigma \\right)$$\n",
    "\n",
    "Think of $\\sigma$ like the accuracy of the dart thrower in our previous example, where lower $\\sigma$ values equate to the dart thrower only hitting a word/token $xi$ if it is very close to $yj$ in word vector space.\n",
    "\n",
    "However, we almost never have a reason to compare any one vector from a sentence $xi$ to every single vector from another sentence/distribution, $yj$. After all, the question we're trying to answer as described in the previous section is \"for each token ($xi$) in the sentence $x$, I want you to tell me if someone in the sample $y$ used the same word, or a synonym for it, in the same way that it was used in $x$.\" Based on this, it’s better to ask, instead how likely is a vector $xi$ might show up in any sample $y$ from the cummulative utterances for the group $Y$, conditioned on what we know about the composition of the sample $y$. To do this, we take the probability of a token $xi$ from the sentence $x$ and the token $yj$ from $y$ that has the lowest CoE with $xi$. This effectively replicates the hypothetical study participant in the example given at the top of this section selecting a word that most closely means the same thing as one of the words ($xi$) from the sentence $x$ and trying to use it to create a new utterance that closely matches $x$ in meaning. Furthermore, if nothing in the distribution $y$ is semantically similar, nor embedded in a similar context as $xi$ is in $x$, then the minimum CoE value will be high (and thus indicates that the token $xi$ doesn't have anything approximating a similar term or usage in $y$). We thus rewrite equation the last equation as follows:\n",
    "\n",
    "$$P(E_{xi} | E_{y}) = P_{\\mathcal{N}_{[0,\\infty]}} \\left( \\min_{j} \\left(CoE(E_{xi},E_{y}) \\right) \\bigg|  \\mu=0., \\sigma \\right)$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 1.4 Entropy across sentences using probability based on their component word vectors\n",
    "\n",
    "Meanwhile, the probability that an individual's message $x$ exhibits convergence with the messaging habits of a groups can be calculated by finding the entropy for $x$ and an imputed sample from the group $y \\in \\lbrace Y | Y_g \\rbrace$.\n",
    "\n",
    "$$H( x ; y ) = -\\sum_i P(E_{xi}|E_{y}) \\log P(E_{xi}|E_{y})$$"
   ]
  },
  {
   "cell_type": "markdown",
   "source": [
    "### 2. Implementation"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "'ssh -'\n",
    "'tmux attach-session -s BERT'\n",
    "'python3 ./indH.py'"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "### 3. Assessment"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "First, I loaded the data from the checkpoint described above."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "import torch\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from SIS.methods.reddit_feminism.stitch_data import get_stitched_data\n",
    "\n",
    "# data_path = \"/Users/zacharyrosen/Desktop/airlock/d/convergence/feminism-menslib-mensrights/women/summaries/posteriors-Feminism.pt\"\n",
    "# ckpt = torch.load(data_path)\n",
    "\n",
    "data_path = \"/Users/zacharyrosen/airlock/d/convergence/feminism-menslib-mensrights/women/summaries/feminism/\"\n",
    "ckpt = get_stitched_data(data_path)\n",
    "\n",
    "total_H = ckpt['M']\n",
    "_ids = ckpt['labels']\n",
    "\n",
    "total_H = total_H.transpose(0,1).transpose(1,2)\n",
    "\n",
    "groups = ['Feminism', 'MensRights', 'MensLib']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "outputs": [
    {
     "data": {
      "text/plain": "torch.Size([935, 3, 200])"
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "total_H.shape"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "#### 3.1 Assessing entropic differences per each sentence in the corpus"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "outputs": [],
   "source": [
    "from scipy.stats import ttest_ind as ttest\n",
    "\n",
    "pvalue, statistic = [], []\n",
    "for i in range(total_H.shape[0]):\n",
    "    r = [\n",
    "        ttest(\n",
    "            total_H[i, 0][~total_H[i, 0].isnan()],\n",
    "            total_H[i, j][~total_H[i, j].isnan()]\n",
    "        ) for j in range(total_H.shape[1])]\n",
    "    pvalue += [np.array([ri.pvalue for ri in r])]\n",
    "    statistic += [np.array([ri.statistic for ri in r])]\n",
    "\n",
    "pvalue, statistic = np.array(pvalue), np.array(statistic)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "outputs": [],
   "source": [
    "minima = [\n",
    "    torch.cat(\n",
    "        [\n",
    "            total_H[i,j][~total_H[i,j].isnan()].mean(axis=-1).view(1,-1) for j in range(len(groups))\n",
    "        ],\n",
    "        dim=-1\n",
    "    ) for i in range(total_H.shape[0])\n",
    "]\n",
    "minima = torch.cat(minima, dim=0).argmin(dim=-1)\n",
    "\n",
    "pct_data, confusion_data, means_data = [], [], []\n",
    "\n",
    "for i, g in enumerate(groups):\n",
    "\n",
    "    p_res = (pvalue[:,i] < .025)\n",
    "    mu_res = (statistic[:,i] < 0)\n",
    "    res =  p_res & mu_res\n",
    "\n",
    "    pct_data += [res.mean(axis=0)]\n",
    "    confusion_data += [(p_res & (minima==i).numpy()).sum(axis=0)]\n",
    "    means_data += [mu_res.mean(axis=0)]\n",
    "\n",
    "results = pd.DataFrame()\n",
    "results['cond'] = groups\n",
    "results['results'] = np.array(pct_data)\n",
    "\n",
    "mean_results = pd.DataFrame()\n",
    "mean_results['cond'] = groups\n",
    "mean_results['results'] = np.array(means_data)\n",
    "\n",
    "confusion = pd.DataFrame()\n",
    "confusion['cond'] = groups\n",
    "confusion['results'] = np.array(confusion_data)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "To interpret the results, higher scores indicate that more examples from the condition in the row passed the test when comparing the reconstruction of terms from the same condition as the row to examples from the condition in the column."
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "outputs": [
    {
     "data": {
      "text/plain": "         cond   results\n0    Feminism  0.000000\n1  MensRights  0.704813\n2     MensLib  0.293048",
      "text/html": "<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>cond</th>\n      <th>results</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>0</th>\n      <td>Feminism</td>\n      <td>0.000000</td>\n    </tr>\n    <tr>\n      <th>1</th>\n      <td>MensRights</td>\n      <td>0.704813</td>\n    </tr>\n    <tr>\n      <th>2</th>\n      <td>MensLib</td>\n      <td>0.293048</td>\n    </tr>\n  </tbody>\n</table>\n</div>"
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "results"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "outputs": [
    {
     "data": {
      "text/plain": "         cond   results\n0    Feminism  0.000000\n1  MensRights  0.850267\n2     MensLib  0.354011",
      "text/html": "<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>cond</th>\n      <th>results</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>0</th>\n      <td>Feminism</td>\n      <td>0.000000</td>\n    </tr>\n    <tr>\n      <th>1</th>\n      <td>MensRights</td>\n      <td>0.850267</td>\n    </tr>\n    <tr>\n      <th>2</th>\n      <td>MensLib</td>\n      <td>0.354011</td>\n    </tr>\n  </tbody>\n</table>\n</div>"
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "mean_results"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "outputs": [
    {
     "data": {
      "text/plain": "         cond  results\n0    Feminism        0\n1  MensRights       28\n2     MensLib      539",
      "text/html": "<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>cond</th>\n      <th>results</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>0</th>\n      <td>Feminism</td>\n      <td>0</td>\n    </tr>\n    <tr>\n      <th>1</th>\n      <td>MensRights</td>\n      <td>28</td>\n    </tr>\n    <tr>\n      <th>2</th>\n      <td>MensLib</td>\n      <td>539</td>\n    </tr>\n  </tbody>\n</table>\n</div>"
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "confusion"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "#### 3.2 By entire comment\n",
    "\n",
    "To calculate the significance for an entire comment we summed the entropy for all the sentences that comprised the comment for each trial number in the data. We then repeated the same testing procedure as performed for the sentence level analysis."
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "outputs": [],
   "source": [
    "comment_H = [total_H[_ids['commentId'].isin([c]).values].sum(axis=0).unsqueeze(0) for c in _ids['commentId'].unique()]\n",
    "comment_H = torch.cat(comment_H,dim=0)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "outputs": [
    {
     "data": {
      "text/plain": "torch.Size([299, 3, 200])"
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "comment_H.shape"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "outputs": [],
   "source": [
    "from scipy.stats import ttest_ind as ttest\n",
    "\n",
    "pvalue, statistic = [], []\n",
    "for i in range(comment_H.shape[0]):\n",
    "    r = [\n",
    "        ttest(\n",
    "            comment_H[i, 0][~comment_H[i, 0].isnan()],\n",
    "            comment_H[i, j][~comment_H[i, j].isnan()]\n",
    "        ) for j in range(comment_H.shape[1])]\n",
    "    pvalue += [np.array([ri.pvalue for ri in r])]\n",
    "    statistic += [np.array([ri.statistic for ri in r])]\n",
    "\n",
    "pvalue, statistic = np.array(pvalue), np.array(statistic)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "outputs": [],
   "source": [
    "minima = [\n",
    "    torch.cat(\n",
    "        [\n",
    "            comment_H[i,j][~comment_H[i,j].isnan()].mean(axis=-1).view(1,-1) for j in range(len(groups))\n",
    "        ],\n",
    "        dim=-1\n",
    "    ) for i in range(comment_H.shape[0])\n",
    "]\n",
    "minima = torch.cat(minima, dim=0).argmin(dim=-1)\n",
    "\n",
    "pct_data, confusion_data, means_data = [], [], []\n",
    "\n",
    "for i, g in enumerate(groups):\n",
    "\n",
    "    p_res = (pvalue[:,i] < .025)\n",
    "    mu_res = (statistic[:,i] < 0)\n",
    "    res =  p_res & mu_res\n",
    "\n",
    "    pct_data += [res.mean(axis=0)]\n",
    "    confusion_data += [(p_res & (minima==i).numpy()).sum(axis=0)]\n",
    "    means_data += [mu_res.mean(axis=0)]\n",
    "\n",
    "results = pd.DataFrame()\n",
    "results['cond'] = groups\n",
    "results['results'] = np.array(pct_data)\n",
    "\n",
    "mean_results = pd.DataFrame()\n",
    "mean_results['cond'] = groups\n",
    "mean_results['results'] = np.array(means_data)\n",
    "\n",
    "confusion = pd.DataFrame()\n",
    "confusion['cond'] = groups\n",
    "confusion['results'] = np.array(confusion_data)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "outputs": [
    {
     "data": {
      "text/plain": "         cond   results\n0    Feminism  0.000000\n1  MensRights  0.806020\n2     MensLib  0.434783",
      "text/html": "<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>cond</th>\n      <th>results</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>0</th>\n      <td>Feminism</td>\n      <td>0.000000</td>\n    </tr>\n    <tr>\n      <th>1</th>\n      <td>MensRights</td>\n      <td>0.806020</td>\n    </tr>\n    <tr>\n      <th>2</th>\n      <td>MensLib</td>\n      <td>0.434783</td>\n    </tr>\n  </tbody>\n</table>\n</div>"
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "results"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "outputs": [
    {
     "data": {
      "text/plain": "         cond   results\n0    Feminism  0.000000\n1  MensRights  0.869565\n2     MensLib  0.468227",
      "text/html": "<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>cond</th>\n      <th>results</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>0</th>\n      <td>Feminism</td>\n      <td>0.000000</td>\n    </tr>\n    <tr>\n      <th>1</th>\n      <td>MensRights</td>\n      <td>0.869565</td>\n    </tr>\n    <tr>\n      <th>2</th>\n      <td>MensLib</td>\n      <td>0.468227</td>\n    </tr>\n  </tbody>\n</table>\n</div>"
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "mean_results"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "outputs": [
    {
     "data": {
      "text/plain": "         cond  results\n0    Feminism        0\n1  MensRights       17\n2     MensLib      140",
      "text/html": "<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>cond</th>\n      <th>results</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>0</th>\n      <td>Feminism</td>\n      <td>0</td>\n    </tr>\n    <tr>\n      <th>1</th>\n      <td>MensRights</td>\n      <td>17</td>\n    </tr>\n    <tr>\n      <th>2</th>\n      <td>MensLib</td>\n      <td>140</td>\n    </tr>\n  </tbody>\n</table>\n</div>"
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "confusion"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "### 3.4 Analysis of Texts in Confusion Matrix"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "outputs": [],
   "source": [
    "__ids = _ids.drop_duplicates(subset=['commentId']).copy()\n",
    "__ids.index = range(len(__ids))\n",
    "\n",
    "texts = pd.read_table(\"/Volumes/ROY/comp_ling/datasci/intergroupEntropy/data/redditany/corpus_with_author_data.tsv\", lineterminator='\\n')\n",
    "texts = texts.loc[~texts['body'].isna()]"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "First, we look at the confused sentences for Feminism -> MensRights"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "outputs": [
    {
     "data": {
      "text/plain": "                    author   subId commentId  \\\n212                   _db_  uschn9   i94hpx3   \n566          RoswalienMath  ulj8z3   i7ya9zb   \n690              dusktrail  ulj8z3   i7won7h   \n1152         shimmerangels  ur0v5s   i8wgknu   \n1197                nona01  ur0v5s   i8x5nnt   \n1198                nona01  ur0v5s   i8x5nnt   \n1205  stayutofwomnbusiness  ur0v5s   i8w3vqm   \n1285            bookluvr83  ukgnfg   i7ph8st   \n1334             Vanilla3K  upgcx5   i8kwai0   \n1352            rougewitch  upgcx5   i8ohncz   \n1623           Shirin00011  ul24nz   i7t6ycm   \n2097                   NaN  uo18l2   i8ppeo4   \n2098                   NaN  uo18l2   i8ppeo4   \n2401            Pyramidddd  un8psc   i89b050   \n2402            Pyramidddd  un8psc   i89b050   \n2405           Englander91  un8psc   i87jhe7   \n2434            nevaneva21  uklhir   i7ynrc6   \n2462   zinfandelbruschetta  urkj9w   i8z0ggo   \n2472            bookluvr83  urkj9w   i8z8arp   \n2473           santana0987  urkj9w   i909fb4   \n\n                                                   body  \n212                                patriarchy dies hard  \n566                                              *might  \n690                                   what do you think  \n1152                                           no words  \n1197                            a bit extreme  isn't it  \n1198                                           edit: /s  \n1205                                        its all men  \n1285                                        ya'll quada  \n1334                the fucking justice system i swear   \n1352                                 men protecting men  \n1623                                                wtf  \n2097    government denies marital rape occurs  natio...  \n2098                    4% of married women are victims  \n2401                                               damn  \n2402                                           powerful  \n2405     are the feminists using mra talking points now  \n2434                                            exactly  \n2462                                    sad and unloved  \n2472        i'd say an asshole  but assholes are useful  \n2473                                             agreed  ",
      "text/html": "<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>author</th>\n      <th>subId</th>\n      <th>commentId</th>\n      <th>body</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>212</th>\n      <td>_db_</td>\n      <td>uschn9</td>\n      <td>i94hpx3</td>\n      <td>patriarchy dies hard</td>\n    </tr>\n    <tr>\n      <th>566</th>\n      <td>RoswalienMath</td>\n      <td>ulj8z3</td>\n      <td>i7ya9zb</td>\n      <td>*might</td>\n    </tr>\n    <tr>\n      <th>690</th>\n      <td>dusktrail</td>\n      <td>ulj8z3</td>\n      <td>i7won7h</td>\n      <td>what do you think</td>\n    </tr>\n    <tr>\n      <th>1152</th>\n      <td>shimmerangels</td>\n      <td>ur0v5s</td>\n      <td>i8wgknu</td>\n      <td>no words</td>\n    </tr>\n    <tr>\n      <th>1197</th>\n      <td>nona01</td>\n      <td>ur0v5s</td>\n      <td>i8x5nnt</td>\n      <td>a bit extreme  isn't it</td>\n    </tr>\n    <tr>\n      <th>1198</th>\n      <td>nona01</td>\n      <td>ur0v5s</td>\n      <td>i8x5nnt</td>\n      <td>edit: /s</td>\n    </tr>\n    <tr>\n      <th>1205</th>\n      <td>stayutofwomnbusiness</td>\n      <td>ur0v5s</td>\n      <td>i8w3vqm</td>\n      <td>its all men</td>\n    </tr>\n    <tr>\n      <th>1285</th>\n      <td>bookluvr83</td>\n      <td>ukgnfg</td>\n      <td>i7ph8st</td>\n      <td>ya'll quada</td>\n    </tr>\n    <tr>\n      <th>1334</th>\n      <td>Vanilla3K</td>\n      <td>upgcx5</td>\n      <td>i8kwai0</td>\n      <td>the fucking justice system i swear</td>\n    </tr>\n    <tr>\n      <th>1352</th>\n      <td>rougewitch</td>\n      <td>upgcx5</td>\n      <td>i8ohncz</td>\n      <td>men protecting men</td>\n    </tr>\n    <tr>\n      <th>1623</th>\n      <td>Shirin00011</td>\n      <td>ul24nz</td>\n      <td>i7t6ycm</td>\n      <td>wtf</td>\n    </tr>\n    <tr>\n      <th>2097</th>\n      <td>NaN</td>\n      <td>uo18l2</td>\n      <td>i8ppeo4</td>\n      <td>government denies marital rape occurs  natio...</td>\n    </tr>\n    <tr>\n      <th>2098</th>\n      <td>NaN</td>\n      <td>uo18l2</td>\n      <td>i8ppeo4</td>\n      <td>4% of married women are victims</td>\n    </tr>\n    <tr>\n      <th>2401</th>\n      <td>Pyramidddd</td>\n      <td>un8psc</td>\n      <td>i89b050</td>\n      <td>damn</td>\n    </tr>\n    <tr>\n      <th>2402</th>\n      <td>Pyramidddd</td>\n      <td>un8psc</td>\n      <td>i89b050</td>\n      <td>powerful</td>\n    </tr>\n    <tr>\n      <th>2405</th>\n      <td>Englander91</td>\n      <td>un8psc</td>\n      <td>i87jhe7</td>\n      <td>are the feminists using mra talking points now</td>\n    </tr>\n    <tr>\n      <th>2434</th>\n      <td>nevaneva21</td>\n      <td>uklhir</td>\n      <td>i7ynrc6</td>\n      <td>exactly</td>\n    </tr>\n    <tr>\n      <th>2462</th>\n      <td>zinfandelbruschetta</td>\n      <td>urkj9w</td>\n      <td>i8z0ggo</td>\n      <td>sad and unloved</td>\n    </tr>\n    <tr>\n      <th>2472</th>\n      <td>bookluvr83</td>\n      <td>urkj9w</td>\n      <td>i8z8arp</td>\n      <td>i'd say an asshole  but assholes are useful</td>\n    </tr>\n    <tr>\n      <th>2473</th>\n      <td>santana0987</td>\n      <td>urkj9w</td>\n      <td>i909fb4</td>\n      <td>agreed</td>\n    </tr>\n  </tbody>\n</table>\n</div>"
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "confused_indexes = __ids['commentId'].loc[(pvalue[:,1] < .025) & (minima == 1).numpy()]\n",
    "\n",
    "texts[['author','subId','commentId','body']].loc[texts['commentId'].isin(confused_indexes)]"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "Then we look at the confused sentences for Feminism -> MensLib"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "outputs": [
    {
     "data": {
      "text/plain": "                   author   subId commentId  \\\n205     notime4urnonsense  uschn9   i92jukq   \n207            mcguinty42  uschn9   i92xudu   \n208            mcguinty42  uschn9   i92xudu   \n454   groundphoenixhogday  ulj8z3   i7wutrt   \n478            cyborgaudi  ulj8z3   i7znfsz   \n...                   ...     ...       ...   \n2751                  NaN  ukbdez   i7rx10x   \n2753            Canvas718  ukbdez   i7vwkpj   \n2754            Canvas718  ukbdez   i7vwkpj   \n2755            Canvas718  ukbdez   i7vwkpj   \n2756            Canvas718  ukbdez   i7vwkpj   \n\n                                                   body  \n205   and men have the audacity to claim that marria...  \n207               that is such an visual way to protest  \n208   i reckon a lot of movements should take notes ...  \n454                             that is a great analogy  \n478   in my experience most of them would say yes to...  \n...                                                 ...  \n2751   surely \"people who menstruate\" is a far more ...  \n2753  trans men are men who may — or may not — have ...  \n2754  in any case  if you support trans people  then...  \n2755        i just couldn’t tell from the original post  \n2756  some people are balking at inclusive language ...  \n\n[602 rows x 4 columns]",
      "text/html": "<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>author</th>\n      <th>subId</th>\n      <th>commentId</th>\n      <th>body</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>205</th>\n      <td>notime4urnonsense</td>\n      <td>uschn9</td>\n      <td>i92jukq</td>\n      <td>and men have the audacity to claim that marria...</td>\n    </tr>\n    <tr>\n      <th>207</th>\n      <td>mcguinty42</td>\n      <td>uschn9</td>\n      <td>i92xudu</td>\n      <td>that is such an visual way to protest</td>\n    </tr>\n    <tr>\n      <th>208</th>\n      <td>mcguinty42</td>\n      <td>uschn9</td>\n      <td>i92xudu</td>\n      <td>i reckon a lot of movements should take notes ...</td>\n    </tr>\n    <tr>\n      <th>454</th>\n      <td>groundphoenixhogday</td>\n      <td>ulj8z3</td>\n      <td>i7wutrt</td>\n      <td>that is a great analogy</td>\n    </tr>\n    <tr>\n      <th>478</th>\n      <td>cyborgaudi</td>\n      <td>ulj8z3</td>\n      <td>i7znfsz</td>\n      <td>in my experience most of them would say yes to...</td>\n    </tr>\n    <tr>\n      <th>...</th>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n    </tr>\n    <tr>\n      <th>2751</th>\n      <td>NaN</td>\n      <td>ukbdez</td>\n      <td>i7rx10x</td>\n      <td>surely \"people who menstruate\" is a far more ...</td>\n    </tr>\n    <tr>\n      <th>2753</th>\n      <td>Canvas718</td>\n      <td>ukbdez</td>\n      <td>i7vwkpj</td>\n      <td>trans men are men who may — or may not — have ...</td>\n    </tr>\n    <tr>\n      <th>2754</th>\n      <td>Canvas718</td>\n      <td>ukbdez</td>\n      <td>i7vwkpj</td>\n      <td>in any case  if you support trans people  then...</td>\n    </tr>\n    <tr>\n      <th>2755</th>\n      <td>Canvas718</td>\n      <td>ukbdez</td>\n      <td>i7vwkpj</td>\n      <td>i just couldn’t tell from the original post</td>\n    </tr>\n    <tr>\n      <th>2756</th>\n      <td>Canvas718</td>\n      <td>ukbdez</td>\n      <td>i7vwkpj</td>\n      <td>some people are balking at inclusive language ...</td>\n    </tr>\n  </tbody>\n</table>\n<p>602 rows × 4 columns</p>\n</div>"
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "confused_indexes = __ids['commentId'].loc[(pvalue[:,2] < .025) & (minima == 2).numpy()]\n",
    "\n",
    "texts[['author','subId','commentId','body']].loc[texts['commentId'].isin(confused_indexes)]"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4. Conclusions\n",
    "\n",
    "The results are interesting. I'll break them up by comparison to each other subreddit here.\n",
    "\n",
    "[_**r/MensRights**_] There is significant evidence that you cannot learn much about the content of a post made to r/Feminism from reading a post in r/MensRights. At a sentence/sub-utterance level, 69.7\\% of examples from r/Feminism have statistically significant higher entropy when trying to recover their meaning from comments posted to r/MensRights when compared to trying to recover the same meaning from comments posted to r/Feminism. 83.3\\% of examples, minimally, have lower entropy in this condition. This discrepency is wider at the comment level, where 83.6\\% of examples from r/Feminism have statistically significant higher entropy when trying to recover their meaning from comments posted to r/MensRights and 91.3\\% of examples, minimally, have lower entropy in this condition.\n",
    "\n",
    "[_**r/MensLib**_] While there are clear differences, it seems possible that you can learn a decent amount about the content of a post made to r/Feminism from reading a post in r/MensLib. At a sentence/sub-utterance level, 27.0\\% of examples from r/Feminism have statistically significant higher entropy when trying to recover their meaning from comments posted to r/MensLib when compared to trying to recover the same meaning from comments posted to r/Feminism. 36.5\\% of examples, minimally, have higher entropy in this condition. This discrepency is not much wider at the comment level, where 34.8\\% of examples from r/Feminism have statistically significant higher entropy when trying to recover their meaning from comments posted to r/MensLib and 41.5\\% of examples, minimally, have lower entropy in this condition.\n",
    "\n",
    "There are two interesting observations that the data lays bare. First, it is clear that there is a rhetorical difference between the firmly Feminist subreddit r/Feminism and the anti-Feminist r/MensRights with respect to how the two groups talk about women. While this is not surprising, it validates that our framework is capable to of identifying real world discursive differences between groups.\n",
    "\n",
    "Interesting, too, is the difference in the quantitative outcomes between our two levels of analysis. That there is a greater difference in entropy at the comment level (comments tend to be comprised of multiple sentences/sub-utterances) would indicate that much of the difference in how online groups conceptualize aspects of a given topic is more observable in larger discursive units.\n",
    "\n",
    "Put another way, the probability that any one claim or statement in an utterance is sampled from a group level communicative norm is not zero, but there's still a good bit of variation. But the probability that the conceptual makeup of a larger discursive unit offered up by an individual is sampled from some set of group norms is significantly higher. In other words, each sentence is more likely to inject of new energy into the system, but people still converge overall to the concepts used by their in-group.\n",
    "\n",
    "The difference in outcomes at varying levels of analysis isn't just an intriguing observation--it can have impacts for the detection of hate speech and other harmful content on the internet. Many if not most hate-speech and/or minsinformation classifiers focus on detecting harmful content at the level of a single claim/sentence. Our results indicate that differences between potentially hateful comment and more acceptable forms of discourse might show up more strongly when taking a step back and looking at a larger proportion of content produced by an individual--how do the myriad things a person has said come together to convey their view of the world? As the old adage goes, \"context is key\" and there is a real possibility that some important context is lost when the focus is on too small a discursive unit.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "#### References\n",
    "\n",
    "Adams, A., Miles, J., Dunbar, N. E., & Giles, H. (2018). Communication accommodation in text messages: Exploring liking, power, and sex as predictors of textisms. The Journal of Social Psychology, 158(4), 474–490. https://doi.org/10.1080/00224545.2017.1421895\n",
    "\n",
    "Dale, R., Duran, N. D., & Coco, M. (2018). Dynamic Natural Language Processing with Recurrence Quantification Analysis. ArXiv:1803.07136 [Cs]. http://arxiv.org/abs/1803.07136\n",
    "\n",
    "de Vries, W., van Cranenburgh, A., & Nissim, M. (2020). What’s so special about BERT’s layers? A closer look at the NLP pipeline in monolingual and multilingual models. Findings of the Association for Computational Linguistics: EMNLP 2020, 4339–4350\n",
    "\n",
    "Palomares, N., Giles, H., Soliz, J., & Gallois, C. (2016). Intergroup Accommodation, Social Categories, and Identities. In H. Giles (Ed.), Communication Accomodation Theory (p. 232).\n",
    "\n",
    "Rosen, Z. (2022). A BERT’s eye view: A “big-data” framework for assessing language convergence and accommodation in large, many-to-many settings. Journal of Language and Social Psychology, 0261927X2210811."
   ]
  },
  {
   "cell_type": "markdown",
   "source": [],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}