{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true,
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "# Convergence as Entropy Minimization Across Lexico-Semantic Choice"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1 Communication Accommodation Theory\n",
    "\n",
    "Lexical alignment/Convergence both describe the tendency for members of a group to converge on similar means of discussing a topic.\n",
    "\n",
    "Similar means can be expressed as a minimization in the entropy between utterances made by group members. As group members A and B sound more similar to one another, you can recover more of a group member A's semantic content from the lexical items in member B's message/utterance/sentence, because you can better predict member A's message just by listening to member B. In other words because they're using similar language to say the same thing the predictability of one utterance when presented with another utterance increases and thus entropy (i.e. how unpredictable two things are based on obersrvations of one or the other) decreases.\n",
    "\n",
    "Similarity in the precise lexico-semantic meaning of two words can be measured using contextual word embeddings. Models like BERT (and it's twin RoBERTa) provide contextually informed word embeddings.\n",
    "\n",
    "The following is a quick analysis attempting to recover Social Identity attributes for speakers based on imputed convergence with another group.\n",
    "\n",
    "Specifically: recovering political party affiliation from tweets discussing immigrants."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 1.1 Generating intial data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "from intergroupEntropy.data.redditany.redo_collection_corpus import *"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 1.2 Word vector representations\n",
    "\n",
    "$$ E_{xi} = wv(i \\in x) $$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "'ssh -'\n",
    "'tmux attach-session -s BERT'\n",
    "'python3 ./reddit_vecs.py'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 1.3 Probability based on word vectors\n",
    "\n",
    "Based on our understanding of how to test conceptual similarities between individuals' utterances and groups', and to get a grip on our BERT-based method, let's imagine that an interlocutor is playing a kind of language reconstruction game that is directly related to the question posed in our description of the Bernoulli process. The interlocutor is given a single utterance from an individual $x$, broken up into tokens $xi$. The interlocutor is then given a set of tokens $j$ from several utterances all taken from a number of members of some group--$y j$. The interlocutor is then asked to take the groups' tokens $yj$ and reconstruct an utterance that means something that is as similar as possible to the sentence $x$. If they can reconstruct a sentence from the groups' tokens $yj$ that means something similar to the utterance $x$, this would effectively answer the question of whether or not\n",
    "\"for each token ($xi$) in the sentence $x$, tell me if someone in the sample $y$ used the same word, or a synonym for it, in the same way that it was used in $x$.\"\n",
    "Furthermore, in this scenario, reconstructed utterances that are more similar in meaning to the original utterance will have lower entropy. Reconstructed utterances that are either less similar or less intelligible will have higher entropy.\n",
    "\n",
    "We start our language game by, first, converting all of the tokens in both $x$ and $y$ to BERT word vectors (Devlin et al. 2019. This will allow us to capture similarity between tokens that are semantically similar but are not a 1:1 mapping of the same word. Let $E_{xi}$ be the set of BERT word vectors for each token $i$ in a sentence $x$ and $E_{yj}$ be the set of BERT word vectors for each token $j$ in a sample $y$ of utterances from a group. The equation \\ref{eq:bert} below shows the process of converting tokens $i \\in x$ to word vectors. Tokens $j \\in y$ are converted to word vectors via the same process.\n",
    "\n",
    "$$E_{xi} = BERT(i \\in x)$$\n",
    "\n",
    "The utility of word vector models is that they represent the meaning of words spatially and, surprisingly, accurately. Even in early, contextually uninformed models, words that are semantically similar to one another based on their word vectors cluster closer together in word vector space (GLoVe: Pennington et al. 2014; Word2Vec: Mikolov et al. 2013; BERT: Devlin et al. 2019). In contextually aware models like BERT and all subsequent transformer models words that have similar word senses cluster separately from other word senses. This allows us to make fine grain distinctions between the different meanings of polysemous words like the many meanings of \"bank, but it also allows us to capture subtle community/group-specific differences in word usage like the differences in the use of the word \"slay\" in example \\ref{} (Devlin et al. 2019). In layman's terms, if a word vector represents the meaning of a word as a point in space, words that are more semantically related to one another will be closer to one another. And if those word vectors are generated by a contextually aware model the closer \\textit{the word senses} of two words are to one another the closer two word vectors will be to one another in vector space. A popular way to measure the proximity of two word vectors to one another is to use Cosine Error (CoE), where a CoE value of 0 indicates that the word vectors for two words in high dimensional space are in a superposition of one another, and 2 means that they are maximally divergent.\n",
    "\n",
    "Think of finding similar words in word vector space like a game of darts, where CoE values that are closer to 0 when comparing a word vector $E_{xi}$ to another word vector $E_{yj}$ indicate that if you threw a dart at $E_{xi}$ you are more likely to accidentally hit the word vector $E_{yj}$ if you miss.\n",
    "\n",
    "Please note however that \\textit{proximity} in vector space is different from a probability, and CoE values are just a scaled measurement of proximity, not the probability that two vectors are the same or similar. An additional step is needed to render CoE values as probabilities that can be used as part of a statistical framework. To convert CoE to a probability, we leverage a half-Gaussian distribution, continuous on an interval of 0 to infinity, with two parameters: (1) a location parameter $\\mu=0.0$ such that as the CoE value for the comparison of two word vectors approaches 0 we have maximum confidence that the two words mean the same thing, and (2) a scale parameter $\\sigma$ that sets a penalty weight for CoE values farther away from 0.\n",
    "$$P(E_{xi} | E_{yj}) = P_{\\mathcal{N}_{[0,\\infty]}}\\left( CoE(E_{xi},E_{yj}) \\bigg|  \\mu=0., \\sigma \\right)$$\n",
    "\n",
    "Think of $\\sigma$ like the accuracy of the dart thrower in our previous example, where lower $\\sigma$ values equate to the dart thrower only hitting a word/token $xi$ if it is very close to $yj$ in word vector space.\n",
    "\n",
    "However, we almost never have a reason to compare any one vector from a sentence $xi$ to every single vector from another sentence/distribution, $yj$. After all, the question we're trying to answer as described in the previous section is \"for each token ($xi$) in the sentence $x$, I want you to tell me if someone in the sample $y$ used the same word, or a synonym for it, in the same way that it was used in $x$.\" Based on this, it’s better to ask, instead how likely is a vector $xi$ might show up in any sample $y$ from the cummulative utterances for the group $Y$, conditioned on what we know about the composition of the sample $y$. To do this, we take the probability of a token $xi$ from the sentence $x$ and the token $yj$ from $y$ that has the lowest CoE with $xi$. This effectively replicates the hypothetical study participant in the example given at the top of this section selecting a word that most closely means the same thing as one of the words ($xi$) from the sentence $x$ and trying to use it to create a new utterance that closely matches $x$ in meaning. Furthermore, if nothing in the distribution $y$ is semantically similar, nor embedded in a similar context as $xi$ is in $x$, then the minimum CoE value will be high (and thus indicates that the token $xi$ doesn't have anything approximating a similar term or usage in $y$). We thus rewrite equation the last equation as follows:\n",
    "\n",
    "$$P(E_{xi} | E_{y}) = P_{\\mathcal{N}_{[0,\\infty]}} \\left( \\min_{j} \\left(CoE(E_{xi},E_{y}) \\right) \\bigg|  \\mu=0., \\sigma \\right)$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 1.4 Entropy across sentences using probability based on their component word vectors\n",
    "\n",
    "Meanwhile, the probability that an individual's message $x$ exhibits convergence with the messaging habits of a groups can be calculated by finding the entropy for $x$ and an imputed sample from the group $y \\in \\lbrace Y | Y_g \\rbrace$.\n",
    "\n",
    "$$H( x ; y ) = -\\sum_i P(E_{xi}|E_{y}) \\log P(E_{xi}|E_{y})$$"
   ]
  },
  {
   "cell_type": "markdown",
   "source": [
    "### 2. Implementation"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "'ssh -'\n",
    "'tmux attach-session -s BERT'\n",
    "'python3 ./indH.py'"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "### 3. Assessment"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "First, I loaded the data from the checkpoint described above."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "import torch\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from SIS.methods.reddit_feminism.stitch_data import get_stitched_data\n",
    "\n",
    "# data_path = \"/Users/zacharyrosen/Desktop/airlock/d/convergence/feminism-menslib-mensrights/women/summaries/posteriors-Feminism.pt\"\n",
    "# ckpt = torch.load(data_path)\n",
    "\n",
    "data_path = \"/Users/zacharyrosen/airlock/d/convergence/feminism-menslib-mensrights/women/summaries/mensrights/\"\n",
    "ckpt = get_stitched_data(data_path)\n",
    "\n",
    "total_H = ckpt['M']\n",
    "_ids = ckpt['labels']\n",
    "\n",
    "total_H = total_H.transpose(0,1).transpose(1,2)\n",
    "\n",
    "groups = ['Feminism', 'MensRights', 'MensLib']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "outputs": [
    {
     "data": {
      "text/plain": "torch.Size([7094, 3, 75])"
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "total_H.shape"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "#### 3.1 Assessing entropic differences per each sentence in the corpus"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "outputs": [],
   "source": [
    "from scipy.stats import ttest_ind as ttest\n",
    "\n",
    "pvalue, statistic = [], []\n",
    "for i in range(total_H.shape[0]):\n",
    "    r = [\n",
    "        ttest(\n",
    "            total_H[i, 1][~total_H[i, 1].isnan()],\n",
    "            total_H[i, j][~total_H[i, j].isnan()]\n",
    "        ) for j in range(total_H.shape[1])]\n",
    "    pvalue += [np.array([ri.pvalue for ri in r])]\n",
    "    statistic += [np.array([ri.statistic for ri in r])]\n",
    "\n",
    "pvalue, statistic = np.array(pvalue), np.array(statistic)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "outputs": [],
   "source": [
    "minima = [\n",
    "    torch.cat(\n",
    "        [\n",
    "            total_H[i,j][~total_H[i,j].isnan()].mean(axis=-1).view(1,-1) for j in range(len(groups))\n",
    "        ],\n",
    "        dim=-1\n",
    "    ) for i in range(total_H.shape[0])\n",
    "]\n",
    "minima = torch.cat(minima, dim=0).argmin(dim=-1)\n",
    "\n",
    "pct_data, confusion_data, means_data = [], [], []\n",
    "\n",
    "for i, g in enumerate(groups):\n",
    "\n",
    "    p_res = (pvalue[:,i] < .025)\n",
    "    mu_res = (statistic[:,i] < 0)\n",
    "    res =  p_res & mu_res\n",
    "\n",
    "    pct_data += [res.mean(axis=0)]\n",
    "    confusion_data += [(p_res & (minima==i).numpy()).sum(axis=0)]\n",
    "    means_data += [mu_res.mean(axis=0)]\n",
    "\n",
    "results = pd.DataFrame()\n",
    "results['cond'] = groups\n",
    "results['results'] = np.array(pct_data)\n",
    "\n",
    "mean_results = pd.DataFrame()\n",
    "mean_results['cond'] = groups\n",
    "mean_results['results'] = np.array(means_data)\n",
    "\n",
    "confusion = pd.DataFrame()\n",
    "confusion['cond'] = groups\n",
    "confusion['results'] = np.array(confusion_data)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "To interpret the results, higher scores indicate that more examples from the condition in the row passed the test when comparing the reconstruction of terms from the same condition as the row to examples from the condition in the column."
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "outputs": [
    {
     "data": {
      "text/plain": "         cond   results\n0    Feminism  0.188469\n1  MensRights  0.000000\n2     MensLib  0.117564",
      "text/html": "<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>cond</th>\n      <th>results</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>0</th>\n      <td>Feminism</td>\n      <td>0.188469</td>\n    </tr>\n    <tr>\n      <th>1</th>\n      <td>MensRights</td>\n      <td>0.000000</td>\n    </tr>\n    <tr>\n      <th>2</th>\n      <td>MensLib</td>\n      <td>0.117564</td>\n    </tr>\n  </tbody>\n</table>\n</div>"
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "results"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "outputs": [
    {
     "data": {
      "text/plain": "         cond   results\n0    Feminism  0.443614\n1  MensRights  0.000000\n2     MensLib  0.196645",
      "text/html": "<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>cond</th>\n      <th>results</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>0</th>\n      <td>Feminism</td>\n      <td>0.443614</td>\n    </tr>\n    <tr>\n      <th>1</th>\n      <td>MensRights</td>\n      <td>0.000000</td>\n    </tr>\n    <tr>\n      <th>2</th>\n      <td>MensLib</td>\n      <td>0.196645</td>\n    </tr>\n  </tbody>\n</table>\n</div>"
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "mean_results"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "outputs": [
    {
     "data": {
      "text/plain": "         cond  results\n0    Feminism      480\n1  MensRights        0\n2     MensLib     4721",
      "text/html": "<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>cond</th>\n      <th>results</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>0</th>\n      <td>Feminism</td>\n      <td>480</td>\n    </tr>\n    <tr>\n      <th>1</th>\n      <td>MensRights</td>\n      <td>0</td>\n    </tr>\n    <tr>\n      <th>2</th>\n      <td>MensLib</td>\n      <td>4721</td>\n    </tr>\n  </tbody>\n</table>\n</div>"
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "confusion"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "#### 3.2 By entire comment\n",
    "\n",
    "To calculate the significance for an entire comment we summed the entropy for all the sentences that comprised the comment for each trial number in the data. We then repeated the same testing procedure as performed for the sentence level analysis."
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "outputs": [],
   "source": [
    "comment_H = [total_H[_ids['commentId'].isin([c]).values].sum(axis=0).unsqueeze(0) for c in _ids['commentId'].unique()]\n",
    "comment_H = torch.cat(comment_H,dim=0)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "outputs": [
    {
     "data": {
      "text/plain": "torch.Size([2357, 3, 75])"
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "comment_H.shape"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "outputs": [],
   "source": [
    "from scipy.stats import ttest_ind as ttest\n",
    "\n",
    "pvalue, statistic = [], []\n",
    "for i in range(comment_H.shape[0]):\n",
    "    r = [\n",
    "        ttest(\n",
    "            comment_H[i, 1][~comment_H[i, 1].isnan()],\n",
    "            comment_H[i, j][~comment_H[i, j].isnan()]\n",
    "        ) for j in range(comment_H.shape[1])]\n",
    "    pvalue += [np.array([ri.pvalue for ri in r])]\n",
    "    statistic += [np.array([ri.statistic for ri in r])]\n",
    "\n",
    "pvalue, statistic = np.array(pvalue), np.array(statistic)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "outputs": [],
   "source": [
    "minima = [\n",
    "    torch.cat(\n",
    "        [\n",
    "            comment_H[i,j][~comment_H[i,j].isnan()].mean(axis=-1).view(1,-1) for j in range(len(groups))\n",
    "        ],\n",
    "        dim=-1\n",
    "    ) for i in range(comment_H.shape[0])\n",
    "]\n",
    "minima = torch.cat(minima, dim=0).argmin(dim=-1)\n",
    "\n",
    "pct_data, confusion_data, means_data = [], [], []\n",
    "\n",
    "for i, g in enumerate(groups):\n",
    "\n",
    "    p_res = (pvalue[:,i] < .025)\n",
    "    mu_res = (statistic[:,i] < 0)\n",
    "    res =  p_res & mu_res\n",
    "\n",
    "    pct_data += [res.mean(axis=0)]\n",
    "    confusion_data += [(p_res & (minima==i).numpy()).sum(axis=0)]\n",
    "    means_data += [mu_res.mean(axis=0)]\n",
    "\n",
    "results = pd.DataFrame()\n",
    "results['cond'] = groups\n",
    "results['results'] = np.array(pct_data)\n",
    "\n",
    "mean_results = pd.DataFrame()\n",
    "mean_results['cond'] = groups\n",
    "mean_results['results'] = np.array(means_data)\n",
    "\n",
    "confusion = pd.DataFrame()\n",
    "confusion['cond'] = groups\n",
    "confusion['results'] = np.array(confusion_data)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "outputs": [
    {
     "data": {
      "text/plain": "         cond   results\n0    Feminism  0.281290\n1  MensRights  0.000000\n2     MensLib  0.188375",
      "text/html": "<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>cond</th>\n      <th>results</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>0</th>\n      <td>Feminism</td>\n      <td>0.281290</td>\n    </tr>\n    <tr>\n      <th>1</th>\n      <td>MensRights</td>\n      <td>0.000000</td>\n    </tr>\n    <tr>\n      <th>2</th>\n      <td>MensLib</td>\n      <td>0.188375</td>\n    </tr>\n  </tbody>\n</table>\n</div>"
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "results"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "outputs": [
    {
     "data": {
      "text/plain": "         cond   results\n0    Feminism  0.532457\n1  MensRights  0.000000\n2     MensLib  0.282987",
      "text/html": "<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>cond</th>\n      <th>results</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>0</th>\n      <td>Feminism</td>\n      <td>0.532457</td>\n    </tr>\n    <tr>\n      <th>1</th>\n      <td>MensRights</td>\n      <td>0.000000</td>\n    </tr>\n    <tr>\n      <th>2</th>\n      <td>MensLib</td>\n      <td>0.282987</td>\n    </tr>\n  </tbody>\n</table>\n</div>"
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "mean_results"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "outputs": [
    {
     "data": {
      "text/plain": "         cond  results\n0    Feminism      164\n1  MensRights        0\n2     MensLib     1390",
      "text/html": "<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>cond</th>\n      <th>results</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>0</th>\n      <td>Feminism</td>\n      <td>164</td>\n    </tr>\n    <tr>\n      <th>1</th>\n      <td>MensRights</td>\n      <td>0</td>\n    </tr>\n    <tr>\n      <th>2</th>\n      <td>MensLib</td>\n      <td>1390</td>\n    </tr>\n  </tbody>\n</table>\n</div>"
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "confusion"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "### 3.4 Analysis of Texts in Confusion Matrix"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "outputs": [],
   "source": [
    "__ids = _ids.drop_duplicates(subset=['commentId']).copy()\n",
    "__ids.index = range(len(__ids))\n",
    "\n",
    "texts = pd.read_table(\"/Volumes/ROY/comp_ling/datasci/intergroupEntropy/data/redditany/corpus_with_author_data.tsv\", lineterminator='\\n')\n",
    "texts = texts.loc[~texts['body'].isna()]"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "First, we look at the confused sentences for MensRights -> Feminism"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "outputs": [
    {
     "data": {
      "text/plain": "                  author   subId commentId  \\\n3436   InformalCriticism  ujz1xi   i7mp3wo   \n3437   InformalCriticism  ujz1xi   i7mp3wo   \n3445         needalife94  ujz1xi   i7n42wo   \n3447         needalife94  ujz1xi   i7n42wo   \n3450         needalife94  ujz1xi   i7n42wo   \n...                  ...     ...       ...   \n21776   AlwaysBeC1imbing  unfjc5   i89g1gn   \n21960   RabbitFromBrazil  unfjc5   i8df13v   \n23730            Halafax  uoel8t   i8e95sj   \n23731            Halafax  uoel8t   i8e95sj   \n23738       Firmus_Eagle  uoel8t   i8i17e9   \n\n                                                    body  \n3436               murder is murder  because we're human  \n3437    pretending murder needs to be parsed is for t...  \n3445                            a fucking five year old   \n3447    that little boy didn't even have a chance to ...  \n3450                                       what the fuck  \n...                                                  ...  \n21776               damn you lot are such fucking losers  \n21960  what you said makes no sense at all and is sim...  \n23730                                             3 days  \n23731   in a country that has special courts just for...  \n23738       i think we can choose to not getting married  \n\n[356 rows x 4 columns]",
      "text/html": "<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>author</th>\n      <th>subId</th>\n      <th>commentId</th>\n      <th>body</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>3436</th>\n      <td>InformalCriticism</td>\n      <td>ujz1xi</td>\n      <td>i7mp3wo</td>\n      <td>murder is murder  because we're human</td>\n    </tr>\n    <tr>\n      <th>3437</th>\n      <td>InformalCriticism</td>\n      <td>ujz1xi</td>\n      <td>i7mp3wo</td>\n      <td>pretending murder needs to be parsed is for t...</td>\n    </tr>\n    <tr>\n      <th>3445</th>\n      <td>needalife94</td>\n      <td>ujz1xi</td>\n      <td>i7n42wo</td>\n      <td>a fucking five year old</td>\n    </tr>\n    <tr>\n      <th>3447</th>\n      <td>needalife94</td>\n      <td>ujz1xi</td>\n      <td>i7n42wo</td>\n      <td>that little boy didn't even have a chance to ...</td>\n    </tr>\n    <tr>\n      <th>3450</th>\n      <td>needalife94</td>\n      <td>ujz1xi</td>\n      <td>i7n42wo</td>\n      <td>what the fuck</td>\n    </tr>\n    <tr>\n      <th>...</th>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n    </tr>\n    <tr>\n      <th>21776</th>\n      <td>AlwaysBeC1imbing</td>\n      <td>unfjc5</td>\n      <td>i89g1gn</td>\n      <td>damn you lot are such fucking losers</td>\n    </tr>\n    <tr>\n      <th>21960</th>\n      <td>RabbitFromBrazil</td>\n      <td>unfjc5</td>\n      <td>i8df13v</td>\n      <td>what you said makes no sense at all and is sim...</td>\n    </tr>\n    <tr>\n      <th>23730</th>\n      <td>Halafax</td>\n      <td>uoel8t</td>\n      <td>i8e95sj</td>\n      <td>3 days</td>\n    </tr>\n    <tr>\n      <th>23731</th>\n      <td>Halafax</td>\n      <td>uoel8t</td>\n      <td>i8e95sj</td>\n      <td>in a country that has special courts just for...</td>\n    </tr>\n    <tr>\n      <th>23738</th>\n      <td>Firmus_Eagle</td>\n      <td>uoel8t</td>\n      <td>i8i17e9</td>\n      <td>i think we can choose to not getting married</td>\n    </tr>\n  </tbody>\n</table>\n<p>356 rows × 4 columns</p>\n</div>"
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "confused_indexes = __ids['commentId'].loc[(pvalue[:,0] < .025) & (minima == 0).numpy()]\n",
    "\n",
    "texts[['author','subId','commentId','body']].loc[texts['commentId'].isin(confused_indexes)]"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "Then we look at the confused sentences for MensRights -> MensLib"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "outputs": [
    {
     "data": {
      "text/plain": "             author   subId commentId  \\\n3439          BDT81  ujz1xi   i7m7k66   \n3440          BDT81  ujz1xi   i7m7k66   \n3463     Neppy_sama  ujz1xi   i7mxzoy   \n3464     Neppy_sama  ujz1xi   i7mxzoy   \n3465     Neppy_sama  ujz1xi   i7mxzoy   \n...             ...     ...       ...   \n23789  Firmus_Eagle  uoel8t   i8i0z1y   \n23790  Firmus_Eagle  uoel8t   i8i0z1y   \n23791  Firmus_Eagle  uoel8t   i8i825b   \n23792  Firmus_Eagle  uoel8t   i8i825b   \n23793  Firmus_Eagle  uoel8t   i8i825b   \n\n                                                    body  \n3439                                  went to look it up  \n3440   check the huge disparity of the wikipedia page...  \n3463   “they [the courts] said he was better off with...  \n3464                            see  women aren't saints  \n3465                       stop ignoring a father's love  \n...                                                  ...  \n23789                    i think working keeps men alive  \n23790                               it is very important  \n23791                                               true  \n23792                   someone has to prioritize things  \n23793  also sex can be totally skipped with women by ...  \n\n[5585 rows x 4 columns]",
      "text/html": "<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>author</th>\n      <th>subId</th>\n      <th>commentId</th>\n      <th>body</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>3439</th>\n      <td>BDT81</td>\n      <td>ujz1xi</td>\n      <td>i7m7k66</td>\n      <td>went to look it up</td>\n    </tr>\n    <tr>\n      <th>3440</th>\n      <td>BDT81</td>\n      <td>ujz1xi</td>\n      <td>i7m7k66</td>\n      <td>check the huge disparity of the wikipedia page...</td>\n    </tr>\n    <tr>\n      <th>3463</th>\n      <td>Neppy_sama</td>\n      <td>ujz1xi</td>\n      <td>i7mxzoy</td>\n      <td>“they [the courts] said he was better off with...</td>\n    </tr>\n    <tr>\n      <th>3464</th>\n      <td>Neppy_sama</td>\n      <td>ujz1xi</td>\n      <td>i7mxzoy</td>\n      <td>see  women aren't saints</td>\n    </tr>\n    <tr>\n      <th>3465</th>\n      <td>Neppy_sama</td>\n      <td>ujz1xi</td>\n      <td>i7mxzoy</td>\n      <td>stop ignoring a father's love</td>\n    </tr>\n    <tr>\n      <th>...</th>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n    </tr>\n    <tr>\n      <th>23789</th>\n      <td>Firmus_Eagle</td>\n      <td>uoel8t</td>\n      <td>i8i0z1y</td>\n      <td>i think working keeps men alive</td>\n    </tr>\n    <tr>\n      <th>23790</th>\n      <td>Firmus_Eagle</td>\n      <td>uoel8t</td>\n      <td>i8i0z1y</td>\n      <td>it is very important</td>\n    </tr>\n    <tr>\n      <th>23791</th>\n      <td>Firmus_Eagle</td>\n      <td>uoel8t</td>\n      <td>i8i825b</td>\n      <td>true</td>\n    </tr>\n    <tr>\n      <th>23792</th>\n      <td>Firmus_Eagle</td>\n      <td>uoel8t</td>\n      <td>i8i825b</td>\n      <td>someone has to prioritize things</td>\n    </tr>\n    <tr>\n      <th>23793</th>\n      <td>Firmus_Eagle</td>\n      <td>uoel8t</td>\n      <td>i8i825b</td>\n      <td>also sex can be totally skipped with women by ...</td>\n    </tr>\n  </tbody>\n</table>\n<p>5585 rows × 4 columns</p>\n</div>"
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "confused_indexes = __ids['commentId'].loc[(pvalue[:,2] < .025) & (minima == 2).numpy()]\n",
    "\n",
    "texts[['author','subId','commentId','body']].loc[texts['commentId'].isin(confused_indexes)]"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4. Conclusions\n",
    "\n",
    "The results are interesting. I'll break them up by comparison to each other subreddit here.\n",
    "\n",
    "[_**r/Feminism**_]\n",
    "\n",
    "[_**r/MensLib**_] \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "#### References\n",
    "\n",
    "Adams, A., Miles, J., Dunbar, N. E., & Giles, H. (2018). Communication accommodation in text messages: Exploring liking, power, and sex as predictors of textisms. The Journal of Social Psychology, 158(4), 474–490. https://doi.org/10.1080/00224545.2017.1421895\n",
    "\n",
    "Dale, R., Duran, N. D., & Coco, M. (2018). Dynamic Natural Language Processing with Recurrence Quantification Analysis. ArXiv:1803.07136 [Cs]. http://arxiv.org/abs/1803.07136\n",
    "\n",
    "de Vries, W., van Cranenburgh, A., & Nissim, M. (2020). What’s so special about BERT’s layers? A closer look at the NLP pipeline in monolingual and multilingual models. Findings of the Association for Computational Linguistics: EMNLP 2020, 4339–4350\n",
    "\n",
    "Palomares, N., Giles, H., Soliz, J., & Gallois, C. (2016). Intergroup Accommodation, Social Categories, and Identities. In H. Giles (Ed.), Communication Accomodation Theory (p. 232).\n",
    "\n",
    "Rosen, Z. (2022). A BERT’s eye view: A “big-data” framework for assessing language convergence and accommodation in large, many-to-many settings. Journal of Language and Social Psychology, 0261927X2210811."
   ]
  },
  {
   "cell_type": "markdown",
   "source": [],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}