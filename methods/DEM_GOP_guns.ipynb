{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true,
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "# Convergence as Entropy Minimization Across Lexico-Semantic Choice"
   ]
  },
  {
   "cell_type": "markdown",
   "source": [
    "### 1 A Theory of Diachronic Language Shift within Groups\n",
    "\n",
    "Linguistic alignment/Convergence both describe the tendency for groups to converge on similar means of discussing a topic.\n",
    "\n",
    "Similar means can be expressed as a minimization in the entropy between utterances made by group members. As group members A and B sound more similar to one another, you can recover more of a group member A's semantic content from the lexical items in member B's message/utterance/sentence, because you can better predict member A's message just by listening to member B. In other words because they're using similar language to say the same thing the predictability of one utterance when presented with another utterance increases and thus entropy (i.e. how unpredictable two things are based on obersrvations of one or the other) decreases.\n",
    "\n",
    "Similarity in the precise lexico-semantic meaning of two words can be measured using contextual word embeddings. Models like BERT provide contextually informed word embeddings.\n",
    "\n",
    "The following is validation on toy data that, yes, there is indeed a difference in the entropy between messages written by members of the same group when compared to members of a different group. We validate this on a subset of 40 post titles discussing COVID-19, evenly sampled from subreddits \"r/covidiots\" and \"r/conspiracy\". No pre-processing was performed on the data."
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "#### 1.1 Generating intial data"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "from convergenceEntropy.data.covidreddit.datacall import *"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "#### 1.2 Word vector representations\n",
    "\n",
    "$$ E_{xi} = wv(i \\in x) $$"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "'python3 ./data/extremismVecsv5.py'"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "#### 1.3 Probability based on word vectors\n",
    "\n",
    "Message histories for individuals is assumed to the be the same when scores for their message history indicate high similarity. Let $x$ be the content of a new message by any one individual, and $h$ be the history of messages by another individual. Assume that for $x$, we are unsure of the individual's purchasing history, while we know the history of purchases made by the individual with history $h$. We calculate the similarity between a thread of messages by quantifying the recurrence (see Dale et al. 2018) between individual message histories:\n",
    "\n",
    "$$ P(E_{xi}|E_{hj}) = P_{\\mathcal{N}}\\left( CoS(E_{xi},E_{hj}) \\bigg| \\mu=1, \\sigma[=.3] \\right) $$"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "import torch\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "s_col = 'subreddit_name'\n",
    "\n",
    "def vec_from_string(dfi):\n",
    "    return torch.FloatTensor(\n",
    "        [\n",
    "            [\n",
    "                float(i) for i in vec.replace('[', '').replace(']','').split(', ')\n",
    "            ] for vec in dfi['vecs'].values\n",
    "        ]\n",
    "    )\n",
    "\n",
    "df = pd.read_table(\"/Volumes/V'GER/comp_ling/DataScideProjects/convergenceEntropy/data/senators458/vecs_538senators-tweets.tsv\")\n",
    "print(list(df))\n",
    "print(df['party'].value_counts())\n",
    "\n",
    "Eu = vec_from_string(df)\n",
    "Eu.shape"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n",
     "is_executing": true
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "Because of how many comparisons there are to make, I had to break the algorithm up. Below we calculate the Cosine Similarity (CoS) for each vector compared to every other vector in the entirety of the dataset."
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "from shared.mod.mutual_information.RQA import *\n",
    "\n",
    "H = hRQA()\n",
    "H.streamCOS(Eu,Eu)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "From there we save that to make later calculations faster."
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "torch.save(\n",
    "    {\n",
    "        'M':H.M,\n",
    "        'df': df[[i for i in list(df) if i not in ['vecs','vec']]],\n",
    "    },\n",
    "    \"/Volumes/V'GER/comp_ling/DataScideProjects/convergenceEntropy/data/senators458/gun-control-ckpt.pt\")"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "#### 1.4 Entropy across sentences using probability based on their component word vectors\n",
    "\n",
    "Meanwhile, the probabililty that an individual's message $x$ exhibits convergence with a historical, known individual/memger of group $h \\in H$ based on their message history is based on (1) how much of $x$'s history, per token, is recoverable from $h$'s history, which is expressed by (2) the amount of information relayed by the closest related lexical item $hj$ to each token $xi$.\n",
    "\n",
    "$$ I( x ; h ) = \\sum_i \\max\\limits_{j \\in h} \\left( log P(E_{xi}|E_{hj}) \\right) $$\n",
    "\n",
    "In the diachronic case, a message $x$ can only be similar to messages in $h$ that were written prior to $x$. Thus, let $\\tau_{x}$ be the time of message $x$, and let $E_{hj}$ contain only tokens pulled from messages occurring prior to $\\tau_x$--i.e.\n",
    "\n",
    "$$E_{hj} = \\bigg\\lbrace wv(j \\in h)\\delta_{\\tau_j < \\tau_x}\\,\\ .\\ .\\ .\\ wv(n \\in h)\\delta_{\\tau_n < \\tau_x}  \\bigg\\rbrace$$"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "First, I loaded the data from the checkpoint described above."
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "import torch\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "s_col = 'party'\n",
    "ckpt = torch.load(\"/Volumes/V'GER/comp_ling/DataScideProjects/convergenceEntropy/data/senators458/gun-control-ckpt.pt\")\n",
    "\n",
    "M, df = ckpt['M'], ckpt['df']\n",
    "ids = df['__id'].values\n",
    "df['party'] = df['party'].replace({'I': 'D'})\n",
    "del ckpt"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "print(list(df))\n",
    "for subreddit in df[s_col].unique():\n",
    "    print('{} \\t {}'.format(subreddit, len(df['__id'].loc[df[s_col].isin([subreddit])].unique())))"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "and then, step-by-step build the entropy matrices, starting with calculating the conditional probability of one vector based on the location of another vector via a Gaussian distribution . . ."
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "comm = np.array([df[s_col].loc[df['__id'].isin([idx])].unique()[0] for idx in np.unique(ids)])\n",
    "\n",
    "#Establish parameters for Gaussian\n",
    "d = torch.distributions.Normal(1,.3,validate_args=False)\n",
    "\n",
    "def entropy(x,y,M=M,d=d):\n",
    "    hxy = M[x][:,y].max(dim=-1).values\n",
    "    hxy = d.log_prob(hxy)\n",
    "    return -(torch.exp(hxy) * hxy).sum()"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "# Hxy = [[d.log_prob(M[ids==i][:,ids==j].max(dim=-1).values) for j in np.unique(ids)] for i in np.unique(ids)]\n",
    "# Hxy = [[-(torch.exp(cell) * cell).sum() for cell in row] for row in Hxy]\n",
    "\n",
    "Hxy = [[entropy(ids==i,ids==j) for j in np.unique(ids)] for i in np.unique(ids)]\n",
    "Hxy = torch.FloatTensor(Hxy)\n",
    "\n",
    "# Saving the subreddit names for later\n",
    "comm = np.array([df[s_col].loc[df['__id'].isin([idx])].unique()[0] for idx in np.unique(ids)])"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "#### Checking model ``accuracy''\n",
    "An important question to consider is how ``accurate'' is our model? Clearly, there are group level phenomena that are being captured, but how well does the model do at simply matching individuals together based on their group affiliation? To measure this, and considering that this is an otherwise unsupervised model, one way to test that the model performs as expected is to ask if, per each comment, the nearest-neighbor-comment (i.e. the comment that has the lowest entropy with the original comment) is from the same group.\n",
    "\n",
    "1. With $\\sigma=.05$, in .917 of cases the nearest neighbor is from the same group.\n",
    "2. With $\\sigma=.1$, in .923 of cases the nearest neighbor is from the same group."
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "y_ = torch.LongTensor([[idx for idx in row if idx != i] for i,row in enumerate(Hxy.argsort(dim=-1))])[:,:5] #note, we do second column because first will always == the message index.\n",
    "(comm == comm[y_[:,0].numpy()]).mean()"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "#### 1.5 Assessing entropic differences between groups\n",
    "\n",
    "Monte Carlo sampling for two conditions: within grouping and outside of group.\n",
    "\n",
    "We break down our hypotheses as follows:\n",
    "\n",
    "- (H1) There is statistically significant, lower entropy between any sampled set of comments posted within a subreddit A comprised of like-minded individuals compared to any sampled set of comments posted to a subreddit B in opposition to A.\n",
    "- (H0) There is no statistically significant difference in entropy between two diametrically opposed subreddits.\n",
    "\n",
    "To test this, picture the following ``game''. Pretend that for each comment in oyur dataset you randomly sample $N$ comments at random from the same subreddit A, and $N$ comments from the opposing subreddit B. We then calculate the entropy between our comment and each comment in the random sample from A, as well as the entropy between our comment and each comment in the random sample from B. We then take the mean entropy between our original comment and the comments from A, and the comments from B separately for comparison. We repeat this process a number of times. Hypothesis H1 is validated if in greater than 5\\% ($\\alpha$) of cases there is lower mean entropy for our initial comment and sample A than there is for our initial comment and sample B. This test is significant if the distribution of means with A and the distribution for means with B is significantly different. Both of these conditions must be true in order to validate H1."
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "### A Permutation test from sampled message histories"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "# (0) negative surprisal\n",
    "# Hxy = d.log_prob(M)\n",
    "\n",
    "# (1) monte carlo procedure\n",
    "N_permutations, xsize, ysize = 100, 10, 20\n",
    "\n",
    "sample_sets = np.array([[j for j in np.unique(ids) if j != i] for i in np.unique(ids)])\n",
    "comm_status = comm[sample_sets]\n",
    "uids = np.array([df['user'].loc[df['__id'].isin([idx])].unique()[0] for idx in np.unique(ids)])\n",
    "uidsset = uids[sample_sets]\n",
    "\n",
    "\n",
    "ML, MR = [],[]\n",
    "for permutation in range(N_permutations):\n",
    "\n",
    "    xml = np.random.choice(np.unique(ids)[comm=='D'], size=(xsize,), replace=False)\n",
    "    xmr = np.random.choice(np.unique(ids)[comm=='R'], size=(xsize,), replace=False)\n",
    "\n",
    "    idxml = df['__id'].isin(xml).values\n",
    "    idxmr = df['__id'].isin(xmr).values\n",
    "\n",
    "    mxml = torch.FloatTensor([df['__id'].values[idxml]==idx for idx in xml])\n",
    "    mxmr = torch.FloatTensor([df['__id'].values[idxmr]==idx for idx in xmr])\n",
    "\n",
    "    ysamples = {idx:\n",
    "        (\n",
    "        np.random.choice(sample_sets[idx][(comm_status[idx] == 'D') & (uidsset[idx] != uids[idx])], size=(ysize,), replace=False),\n",
    "        np.random.choice(sample_sets[idx][(comm_status[idx] == 'R') & (uidsset[idx] != uids[idx])], size=(ysize,), replace=False)\n",
    "        ) for idx in xml.tolist()+xmr.tolist()}\n",
    "\n",
    "    ysamples ={k: (df['__id'].isin(v[0]).values, df['__id'].isin(v[1]).values) for k,v in ysamples.items()}\n",
    "\n",
    "    ml, mr = [],[]\n",
    "    for x in xml:\n",
    "        xi = M[ids==x]\n",
    "        ml += [torch.cat([xi[:,ysamples[x][0]].max(dim=-1).values.unsqueeze(-1),\n",
    "                          xi[:,ysamples[x][1]].max(dim=-1).values.unsqueeze(-1)], dim=-1)]\n",
    "\n",
    "    for x in xmr:\n",
    "        xi = M[ids == x]\n",
    "        mr += [torch.cat([xi[:, ysamples[x][0]].max(dim=-1).values.unsqueeze(-1),\n",
    "                          xi[:, ysamples[x][1]].max(dim=-1).values.unsqueeze(-1)], dim=-1)]\n",
    "\n",
    "    ml, mr = torch.cat(ml, dim=0), torch.cat(mr, dim=0)\n",
    "    ml, mr = d.log_prob(ml), d.log_prob(mr)\n",
    "    ml, mr = -(torch.exp(ml) * ml), -(torch.exp(mr) * mr)\n",
    "    ml, mr = (mxml.unsqueeze(-1) * ml).sum(dim=1), (mxmr.unsqueeze(-1) * mr).sum(dim=1)\n",
    "\n",
    "    ML += [ml]\n",
    "    MR += [mr]\n",
    "\n",
    "ML, MR = torch.cat(ML,dim=0), torch.cat(MR,dim=0)\n",
    "\n",
    "# Printing means and medians as test of normalcy\n",
    "(ML[:,0].mean(), ML[:,0].median()), (ML[:,1].mean(), ML[:,1].median()), (MR[:,0].mean(), MR[:,0].median()), (MR[:,1].mean(), MR[:,1].median())"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "And testing the statistical significance of these distributions . . ."
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "from scipy.stats import ttest_ind as ttest\n",
    "\n",
    "ttest(ML[:,0].numpy(), ML[:,1].numpy()),ttest(MR[:,0].numpy(), MR[:,1].numpy())"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "### 2 Conclusions\n",
    "\n",
    "What's provided here is a mock up example with real (albeit a fractional amount) data validating that within group communication exhibits lower entropy than intergroup communication. The methods proposed here can be used in a variety of applications, ranging from quantifying convergence within groups without prior definition of lexical dictionaries to, per person, measuring convergence with normative group communication styles over time (message-by-message)"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "#### References\n",
    "\n",
    "Adams, A., Miles, J., Dunbar, N. E., & Giles, H. (2018). Communication accommodation in text messages: Exploring liking, power, and sex as predictors of textisms. The Journal of Social Psychology, 158(4), 474–490. https://doi.org/10.1080/00224545.2017.1421895\n",
    "\n",
    "Dale, R., Duran, N. D., & Coco, M. (2018). Dynamic Natural Language Processing with Recurrence Quantification Analysis. ArXiv:1803.07136 [Cs]. http://arxiv.org/abs/1803.07136\n",
    "\n",
    "de Vries, W., van Cranenburgh, A., & Nissim, M. (2020). What’s so special about BERT’s layers? A closer look at the NLP pipeline in monolingual and multilingual models. Findings of the Association for Computational Linguistics: EMNLP 2020, 4339–4350\n",
    "\n",
    "Palomares, N., Giles, H., Soliz, J., & Gallois, C. (2016). Intergroup Accommodation, Social Categories, and Identities. In H. Giles (Ed.), Communication Accomodation Theory (p. 232).\n",
    "\n",
    "Rosen, Z. (2022). A BERT’s eye view: A “big-data” framework for assessing language convergence and accommodation in large, many-to-many settings. Journal of Language and Social Psychology, 0261927X2210811."
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}