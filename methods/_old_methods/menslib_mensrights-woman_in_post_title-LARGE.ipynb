{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true,
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "# Convergence as Entropy Minimization Across Lexico-Semantic Choice"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1 A Theory of Diachronic Language Shift within Groups\n",
    "\n",
    "Linguistic alignment/Convergence both describe the tendency for groups to converge on similar means of discussing a topic.\n",
    "\n",
    "Similar means can be expressed as a minimization in the entropy between utterances made by group members. As group members A and B sound more similar to one another, you can recover more of a group member A's semantic content from the lexical items in member B's message/utterance/sentence, because you can better predict member A's message just by listening to member B. In other words because they're using similar language to say the same thing the predictability of one utterance when presented with another utterance increases and thus entropy (i.e. how unpredictable two things are based on obersrvations of one or the other) decreases.\n",
    "\n",
    "Similarity in the precise lexico-semantic meaning of two words can be measured using contextual word embeddings. Models like BERT provide contextually informed word embeddings.\n",
    "\n",
    "The following is validation on toy data that, yes, there is indeed a difference in the entropy between messages written by members of the same group when compared to members of a different group. We validate this on a subset of 40 post titles discussing COVID-19, evenly sampled from subreddits \"r/covidiots\" and \"r/conspiracy\". No pre-processing was performed on the data."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 1.1 Generating intial data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "from convergenceEntropy.data.redditany.corpusv2 import *\n",
    "from convergenceEntropy.data.redditany.split_comments import *\n",
    "dfs2 = split_values(dfs, 'comment')\n",
    "dfs2.to_csv(output_file.replace('.csv', '_split-comments.csv'),index=False, encoding='utf-8')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 1.2 Word vector representations\n",
    "\n",
    "$$ E_{xi} = wv(i \\in x) $$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "'python3 ./data/extremismVecsv4.py'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 1.3 Probability based on word vectors\n",
    "\n",
    "Message histories for individuals is assumed to the be the same when scores for their message history indicate high similarity. Let $x$ be the content of a new message by any one individual, and $h$ be the history of messages by another individual. Assume that for $x$, we are unsure of the individual's purchasing history, while we know the history of purchases made by the individual with history $h$. We calculate the similarity between a thread of messages by quantifying the recurrence (see Dale et al. 2018) between individual message histories:\n",
    "\n",
    "$$ P(E_{xi}|E_{hj}) = P_{\\mathcal{N}}\\left( CoS(E_{xi},E_{hj}) \\bigg| \\mu=1, \\sigma[=.3] \\right) $$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['__id', 'subreddit_name', 'post_id', 'post_title', 'index', 'pattern_found', 'tokens', 'vecs']\n",
      "mensrights    67294\n",
      "feminism      16073\n",
      "menslib        5222\n",
      "Name: subreddit_name, dtype: int64\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "cos = nn.CosineSimilarity(dim=-1)\n",
    "s_col = 'subreddit_name'\n",
    "\n",
    "def vec_from_string(dfi):\n",
    "    return torch.FloatTensor(\n",
    "        [\n",
    "            [\n",
    "                float(i) for i in vec.replace('[', '').replace(']','').split(', ')\n",
    "            ] for vec in dfi['vecs'].values\n",
    "        ]\n",
    "    )\n",
    "\n",
    "groups = ['menslib', 'feminism', 'mensrights']\n",
    "\n",
    "df = pd.read_table(\"/Volumes/V'GER/comp_ling/DataScideProjects/convergenceEntropy/data/redditany/three_groups/woman_post_title/vecs_feminism-mensrights-menslib-women_post_title.tsv\")\n",
    "print(list(df))\n",
    "print(df['subreddit_name'].value_counts())\n",
    "df = df.loc[\n",
    "    df[s_col].isin(groups)\n",
    "    # & df['pattern_found'].values\n",
    "]\n",
    "df.index=range(len(df))\n",
    "df['--id'] = df['__id'].values\n",
    "\n",
    "update_id_dic = {i:idx for idx,i in enumerate(np.unique(df['__id'].values))}\n",
    "df['__id'] = df['__id'].replace(update_id_dic)\n",
    "\n",
    "ids = np.unique(df['__id'].values)\n",
    "comm = np.array([df[s_col].loc[df['__id'].isin([idx])].unique()[0] for idx in ids])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['__id', 'subreddit_name', 'post_id', 'post_title', 'index', 'pattern_found', 'tokens', 'vecs', '--id']\n",
      "feminism \t 897\n",
      "mensrights \t 4051\n",
      "menslib \t 244\n"
     ]
    }
   ],
   "source": [
    "print(list(df))\n",
    "for subreddit in df['subreddit_name'].unique():\n",
    "    print('{} \\t {}'.format(subreddit, len(df['__id'].loc[df['subreddit_name'].isin([subreddit])].unique())))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([88589, 1536])"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "Eu = vec_from_string(df)\n",
    "del df['vecs']\n",
    "Eu.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "#Establish parameters for Gaussian\n",
    "d = torch.distributions.Normal(1,.3,validate_args=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 1.4 Entropy across sentences using probability based on their component word vectors\n",
    "\n",
    "Meanwhile, the probabililty that an individual's message $x$ exhibits convergence with a historical, known individual/memger of group $h \\in H$ based on their message history is based on (1) how much of $x$'s history, per token, is recoverable from $h$'s history, which is expressed by (2) the amount of information relayed by the closest related lexical item $hj$ to each token $xi$.\n",
    "\n",
    "$$ I( x ; h ) = \\sum_i \\max\\limits_{j \\in h} \\left( log P(E_{xi}|E_{hj}) \\right) $$\n",
    "\n",
    "In the diachronic case, a message $x$ can only be similar to messages in $h$ that were written prior to $x$. Thus, let $\\tau_{x}$ be the time of message $x$, and let $E_{hj}$ contain only tokens pulled from messages occurring prior to $\\tau_x$--i.e.\n",
    "\n",
    "$$E_{hj} = \\bigg\\lbrace wv(j \\in h)\\delta_{\\tau_j < \\tau_x}\\,\\ .\\ .\\ .\\ wv(n \\in h)\\delta_{\\tau_n < \\tau_x}  \\bigg\\rbrace$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "and then, step-by-step build the entropy matrices, starting with calculating the conditional probability of one vector based on the location of another vector via a Gaussian distribution . . ."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 1.5 Assessing entropic differences between groups\n",
    "\n",
    "Monte Carlo sampling for two conditions: within grouping and outside of group.\n",
    "\n",
    "We break down our hypotheses as follows:\n",
    "\n",
    "- (H1) There is statistically significant, lower entropy between any sampled set of comments posted within a subreddit A comprised of like-minded individuals compared to any sampled set of comments posted to a subreddit B in opposition to A.\n",
    "- (H0) There is no statistically significant difference in entropy between two diametrically opposed subreddits.\n",
    "\n",
    "To test this, picture the following ``game''. Pretend that for each comment in oyur dataset you randomly sample $N$ comments at random from the same subreddit A, and $N$ comments from the opposing subreddit B. We then calculate the entropy between our comment and each comment in the random sample from A, as well as the entropy between our comment and each comment in the random sample from B. We then take the mean entropy between our original comment and the comments from A, and the comments from B separately for comparison. We repeat this process a number of times. Hypothesis H1 is validated if in greater than 5\\% ($\\alpha$) of cases there is lower mean entropy for our initial comment and sample A than there is for our initial comment and sample B. This test is significant if the distribution of means with A and the distribution for means with B is significantly different. Both of these conditions must be true in order to validate H1."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### A Permutation test from sampled message histories"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "permutation 10/50\n",
      "permutation 20/50\n",
      "permutation 30/50\n",
      "permutation 40/50\n",
      "permutation 50/50\n"
     ]
    }
   ],
   "source": [
    "# (1) monte carlo procedure\n",
    "# N_permutations, xsize, ysize = 1000, 20, 20\n",
    "N_permutations, xsize, ysize = 50, 200, 100\n",
    "\n",
    "sample_sets = np.array([[j for j in np.unique(ids) if j != i] for i in np.unique(ids)])\n",
    "comm_status = comm[sample_sets]\n",
    "\n",
    "ids = df['__id'].values\n",
    "\n",
    "M = []\n",
    "for permutation in range(N_permutations):\n",
    "\n",
    "    #dictionary of sampled comments of shape group x sample_size\n",
    "    xm = {group: np.random.choice(np.unique(ids)[comm==group], size=(xsize,), replace=False) for group in groups}\n",
    "\n",
    "    #dictionary of indeces for each sample per group of shape group x total_number_of_vecs\n",
    "    idxm = {group: df['__id'].isin(xm[group]).values for group in groups}\n",
    "\n",
    "    #dictionary of masks for each sample group for summation of shape group x sample_size x total_number_of_vecs\n",
    "    mxm = {group: torch.FloatTensor([df['__id'].values[idxm[group]]==idx for idx in xm[group]]) for group in groups}\n",
    "\n",
    "    #dictionary of y-axis samples for each sampled conversation of shape sample_conversation x group x y_axis_sample_size\n",
    "    ysamples = {idx:\n",
    "                    { group:\n",
    "                          np.random.choice(sample_sets[idx][(comm_status[idx] == group)], size=(ysize,), replace=False)\n",
    "                      for group in groups }\n",
    "        for idx in sum([xm[group].tolist() for group in groups],[])}\n",
    "\n",
    "    key = list(ysamples.keys())[0]\n",
    "\n",
    "    #dictionary of y-axis sample indeces for each sampled conversation of shape sample_conversation x group x total_number_of_vecs\n",
    "    ysamples = { k:\n",
    "                    { group:\n",
    "                          df['__id'].isin(v[group]).values\n",
    "                      for group in groups }\n",
    "                for k,v in ysamples.items() }\n",
    "\n",
    "    m = []\n",
    "    for group in groups:\n",
    "        #calculating response per each group of shape idxm[group] x ysamples[group,sample_conversation,groupY] and taking the max for each.\n",
    "        r = torch.cat([\n",
    "            torch.cat([\n",
    "                cos(\n",
    "                    Eu[ids==x].unsqueeze(1),\n",
    "                    Eu[ysamples[x][groupy]]).max(dim=-1).values.unsqueeze(-1)\n",
    "                for groupy in groups], dim=-1)\n",
    "            for x in xm[group]],dim=0)\n",
    "\n",
    "        r = d.log_prob(r)\n",
    "        r = -(torch.exp(r) * r)\n",
    "        r = (mxm[group].unsqueeze(-1) * r).sum(dim=1)\n",
    "        m += [r.unsqueeze(0)]\n",
    "\n",
    "    m = torch.cat(m, dim=0)\n",
    "    M += [m.unsqueeze(0)]\n",
    "\n",
    "    if ((permutation+1) % 10) == 0:\n",
    "        print('permutation {}/{}'.format(permutation+1, N_permutations))\n",
    "\n",
    "# M is a matrix of shape trials x groups x sample_size x group_history_sampled_from\n",
    "M  = torch.cat(M,dim=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "menslib:menslib \t :: 4.324417591094971 3.6158299446105957\n",
      "menslib:feminism \t :: 5.093105316162109 4.231527805328369\n",
      "menslib:mensrights \t :: 5.057657718658447 4.188711166381836\n",
      "=======][=======\n",
      "feminism:menslib \t :: 4.243061542510986 3.383118152618408\n",
      "feminism:feminism \t :: 3.857571840286255 3.1290817260742188\n",
      "feminism:mensrights \t :: 4.275981903076172 3.411529064178467\n",
      "=======][=======\n",
      "mensrights:menslib \t :: 3.801950693130493 3.0633301734924316\n",
      "mensrights:feminism \t :: 3.8546786308288574 3.1316821575164795\n",
      "mensrights:mensrights \t :: 3.757641315460205 3.0317273139953613\n",
      "=======][=======\n"
     ]
    }
   ],
   "source": [
    "#Printing medians and means\n",
    "for i, group in enumerate(groups):\n",
    "    for j,comparison in enumerate(groups):\n",
    "        res = M[:,i,:,j].reshape(-1)\n",
    "        print('{}:{} \\t :: {} {}'.format(group,comparison,res.mean(), res.median()))\n",
    "    print('=======][=======')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "And testing the statistical significance of these results . . ."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(ttest) menslib:feminism \t :: Ttest_indResult(statistic=-15.972620796757303, pvalue=4.4711840432767804e-57)\n",
      "(ttest) menslib:mensrights \t :: Ttest_indResult(statistic=-15.256106879018825, pvalue=2.9528948967259668e-52)\n",
      "=======][=======\n",
      "(ttest) feminism:menslib \t :: Ttest_indResult(statistic=-7.672250985310879, pvalue=1.767278247455216e-14)\n",
      "(ttest) feminism:mensrights \t :: Ttest_indResult(statistic=-8.304318520333238, pvalue=1.0671144885415052e-16)\n",
      "=======][=======\n",
      "(ttest) mensrights:menslib \t :: Ttest_indResult(statistic=-0.9515126309586427, pvalue=0.3413557093157765)\n",
      "(ttest) mensrights:feminism \t :: Ttest_indResult(statistic=-2.060683405155173, pvalue=0.039346156629148685)\n",
      "=======][=======\n"
     ]
    }
   ],
   "source": [
    "from scipy.stats import ttest_ind as ttest\n",
    "\n",
    "for i, group in enumerate(groups):\n",
    "    for j,comparison in enumerate(groups):\n",
    "        if group != comparison:\n",
    "            sample1 = M[:,i,:,i].reshape(-1).numpy()\n",
    "            sample2 = M[:,i,:,j].reshape(-1).numpy()\n",
    "            print('(ttest) {}:{} \\t :: {}'.format(group,comparison,ttest(sample1,sample2)))\n",
    "    print('=======][=======')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "torch.save({'M': M}, \"/Volumes/V'GER/comp_ling/DataScideProjects/convergenceEntropy/data/redditany/three_groups/woman_post_title/sampled-ckpt-y100.pt\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2 Conclusions\n",
    "\n",
    "What's provided here is a mock up example with real (albeit a fractional amount) data validating that within group communication exhibits lower entropy than intergroup communication. The methods proposed here can be used in a variety of applications, ranging from quantifying convergence within groups without prior definition of lexical dictionaries to, per person, measuring convergence with normative group communication styles over time (message-by-message)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "#### References\n",
    "\n",
    "Adams, A., Miles, J., Dunbar, N. E., & Giles, H. (2018). Communication accommodation in text messages: Exploring liking, power, and sex as predictors of textisms. The Journal of Social Psychology, 158(4), 474–490. https://doi.org/10.1080/00224545.2017.1421895\n",
    "\n",
    "Dale, R., Duran, N. D., & Coco, M. (2018). Dynamic Natural Language Processing with Recurrence Quantification Analysis. ArXiv:1803.07136 [Cs]. http://arxiv.org/abs/1803.07136\n",
    "\n",
    "de Vries, W., van Cranenburgh, A., & Nissim, M. (2020). What’s so special about BERT’s layers? A closer look at the NLP pipeline in monolingual and multilingual models. Findings of the Association for Computational Linguistics: EMNLP 2020, 4339–4350\n",
    "\n",
    "Palomares, N., Giles, H., Soliz, J., & Gallois, C. (2016). Intergroup Accommodation, Social Categories, and Identities. In H. Giles (Ed.), Communication Accomodation Theory (p. 232).\n",
    "\n",
    "Rosen, Z. (2022). A BERT’s eye view: A “big-data” framework for assessing language convergence and accommodation in large, many-to-many settings. Journal of Language and Social Psychology, 0261927X2210811."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Previous Sampling method"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "cos = nn.CosineSimilarity(dim=-1)\n",
    "s_col = 'subreddit_name'\n",
    "\n",
    "def vec_from_string(dfi):\n",
    "    return torch.FloatTensor(\n",
    "        [\n",
    "            [\n",
    "                float(i) for i in vec.replace('[', '').replace(']','').split(', ')\n",
    "            ] for vec in dfi['vecs'].values\n",
    "        ]\n",
    "    )\n",
    "\n",
    "groups = ['feminism', 'mensrights']\n",
    "\n",
    "df = pd.read_table(\"/Volumes/V'GER/comp_ling/DataScideProjects/convergenceEntropy/data/redditany/three_groups/woman_post_title/vecs_feminism-mensrights-menslib-women_post_title.tsv\")\n",
    "print(list(df))\n",
    "print(df['subreddit_name'].value_counts())\n",
    "df = df.loc[df[s_col].isin(groups) & df['pattern_found'].values]\n",
    "df.index=range(len(df))\n",
    "df['--id'] = df['__id'].values\n",
    "\n",
    "update_id_dic = {i:idx for idx,i in enumerate(np.unique(df['__id'].values))}\n",
    "df['__id'] = df['__id'].replace(update_id_dic)\n",
    "\n",
    "ids = np.unique(df['__id'].values)\n",
    "comm = np.array([df[s_col].loc[df['__id'].isin([idx])].unique()[0] for idx in ids])\n",
    "\n",
    "print(list(df))\n",
    "for subreddit in df['subreddit_name'].unique():\n",
    "    print('{} \\t {}'.format(subreddit, len(df['__id'].loc[df['subreddit_name'].isin([subreddit])].unique())))\n",
    "\n",
    "Eu = vec_from_string(df)\n",
    "del df['vecs']\n",
    "Eu.shape\n",
    "\n",
    "#Establish parameters for Gaussian\n",
    "d = torch.distributions.Normal(1, .3, validate_args=False)\n",
    "\n",
    "# (0) defining groups\n",
    "\n",
    "\n",
    "\n",
    "# (1) monte carlo procedure\n",
    "N_permutations, xsize, ysize = 100, 10, 20\n",
    "\n",
    "sample_sets = np.array([[j for j in np.unique(ids) if j != i] for i in np.unique(ids)])\n",
    "comm_status = comm[sample_sets]\n",
    "\n",
    "ids = df['__id'].values\n",
    "\n",
    "ML, MR = [],[]\n",
    "for permutation in range(N_permutations):\n",
    "\n",
    "    xml = np.random.choice(np.unique(ids)[comm==groups[0]], size=(xsize,), replace=False)\n",
    "    xmr = np.random.choice(np.unique(ids)[comm==groups[1]], size=(xsize,), replace=False)\n",
    "\n",
    "    idxml = df['__id'].isin(xml).values\n",
    "    idxmr = df['__id'].isin(xmr).values\n",
    "\n",
    "    mxml = torch.FloatTensor([df['__id'].values[idxml]==idx for idx in xml])\n",
    "    mxmr = torch.FloatTensor([df['__id'].values[idxmr]==idx for idx in xmr])\n",
    "\n",
    "    ysamples = {idx:\n",
    "        (\n",
    "        np.random.choice(sample_sets[idx][(comm_status[idx] == groups[0]) ], size=(ysize,), replace=False),\n",
    "        np.random.choice(sample_sets[idx][(comm_status[idx] == groups[1])], size=(ysize,), replace=False)\n",
    "        ) for idx in xml.tolist()+xmr.tolist()}\n",
    "\n",
    "    ysamples ={k: (df['__id'].isin(v[0]).values, df['__id'].isin(v[1]).values) for k,v in ysamples.items()}\n",
    "\n",
    "    ml, mr = [],[]\n",
    "    for x in xml:\n",
    "        xil = cos(\n",
    "            Eu[ids==x].unsqueeze(1),\n",
    "            Eu[ysamples[x][0]])\n",
    "        xir = cos(\n",
    "            Eu[ids==x].unsqueeze(1),\n",
    "            Eu[ysamples[x][1]])\n",
    "\n",
    "        ml += [torch.cat([xil.max(dim=-1).values.unsqueeze(-1),\n",
    "                          xir.max(dim=-1).values.unsqueeze(-1)], dim=-1)]\n",
    "\n",
    "    for x in xmr:\n",
    "        xil = cos(Eu[ids==x].unsqueeze(1), Eu[ysamples[x][0]])\n",
    "        xir = cos(Eu[ids==x].unsqueeze(1), Eu[ysamples[x][1]])\n",
    "\n",
    "        mr += [torch.cat([xil.max(dim=-1).values.unsqueeze(-1),\n",
    "                          xir.max(dim=-1).values.unsqueeze(-1)], dim=-1)]\n",
    "\n",
    "    ml, mr = torch.cat(ml, dim=0), torch.cat(mr, dim=0)\n",
    "    ml, mr = d.log_prob(ml), d.log_prob(mr)\n",
    "    ml, mr = -(torch.exp(ml) * ml), -(torch.exp(mr) * mr)\n",
    "    ml, mr = (mxml.unsqueeze(-1) * ml).sum(dim=1), (mxmr.unsqueeze(-1) * mr).sum(dim=1)\n",
    "\n",
    "    ML += [ml]\n",
    "    MR += [mr]\n",
    "\n",
    "    print('permutation {}/{}'.format(permutation+1, N_permutations))\n",
    "\n",
    "ML, MR = torch.cat(ML,dim=0), torch.cat(MR,dim=0)\n",
    "\n",
    "# Printing means and medians as test of normalcy\n",
    "(ML[:,0].mean(), ML[:,0].median()), (ML[:,1].mean(), ML[:,1].median()), (MR[:,0].mean(), MR[:,0].median()), (MR[:,1].mean(), MR[:,1].median())"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
